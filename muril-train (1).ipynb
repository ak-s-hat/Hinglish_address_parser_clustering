{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install transformers\n",
    "\n",
    "import os\n",
    "os.kill(os.getpid(), 9)  # üîÅ Restarts the Python kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T18:33:33.584191Z",
     "iopub.status.busy": "2025-07-16T18:33:33.583911Z",
     "iopub.status.idle": "2025-07-16T18:34:10.431968Z",
     "shell.execute_reply": "2025-07-16T18:34:10.431049Z",
     "shell.execute_reply.started": "2025-07-16T18:33:33.584171Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.53.2\n",
      "CUDA Available? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33073</td>\n",
       "      <td>33073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>32108</td>\n",
       "      <td>21965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>mcd zonal office , shdn zone , near shyamlal c...</td>\n",
       "      <td>B-HOUSE_NUMBER I-HOUSE_NUMBER I-HOUSE_NUMBER I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "count                                               33073   \n",
       "unique                                              32108   \n",
       "top     mcd zonal office , shdn zone , near shyamlal c...   \n",
       "freq                                                   15   \n",
       "\n",
       "                                                   labels  \n",
       "count                                               33073  \n",
       "unique                                              21965  \n",
       "top     B-HOUSE_NUMBER I-HOUSE_NUMBER I-HOUSE_NUMBER I...  \n",
       "freq                                                  155  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "#!pip install -q datasets seqeval\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "print(\"CUDA Available?\", torch.cuda.is_available())\n",
    "# ‚úÖ Load your data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:/ML/grok_adddress_parser/grok_address_parser-env/bio_tagged_data_108361_141435_eval.csv\")  # Make sure labels are space-separated strings\n",
    "df.rename(columns = {'sentence':'text', 'tags':'labels'}, inplace=True)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T18:34:14.634481Z",
     "iopub.status.busy": "2025-07-16T18:34:14.633926Z",
     "iopub.status.idle": "2025-07-16T18:34:15.046445Z",
     "shell.execute_reply": "2025-07-16T18:34:15.045823Z",
     "shell.execute_reply.started": "2025-07-16T18:34:14.634449Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "delhi\n",
      "gali\n",
      "f\n",
      "nagar\n",
      "vihar\n",
      "floor\n",
      "kh\n",
      "a\n",
      "g\n",
      "b\n",
      "karawal\n",
      "block\n",
      "c\n",
      "d\n",
      "e\n",
      "old\n",
      "laxmi\n",
      "sonia\n",
      "s\n",
      "ground\n",
      "h\n",
      "mandawali\n",
      "pur\n",
      "first\n",
      "fazalpur\n",
      "colony\n",
      "plot\n",
      "west\n",
      "shakarpur\n",
      "mustafabad\n",
      "road\n",
      "shiv\n",
      "pusta\n",
      "second\n",
      "ram\n",
      "new\n",
      "nehru\n",
      "main\n",
      "third\n",
      "khasra\n",
      "vinod\n",
      "school\n",
      "extn\n",
      "near\n",
      "ganesh\n",
      "hno\n",
      "shri\n",
      "ph\n",
      "khas\n",
      "pandav\n",
      "khajoori\n",
      "k\n",
      "khno\n",
      "park\n",
      "bhagirathi\n",
      "side\n",
      "rajeev\n",
      "extension\n",
      "part\n",
      "j\n",
      "gandhi\n",
      "johri\n",
      "dayalpur\n",
      "chand\n",
      "bagh\n",
      "marg\n",
      "ngr\n",
      "dayal\n",
      "puri\n"
     ]
    }
   ],
   "source": [
    "#to count the top 100 most common tokenns whose frequencies are above 50\n",
    "from collections import Counter\n",
    "# For example, assume addresses are in df[\"address\"]\n",
    "tokens = [tok for addr in df[\"text\"] for tok in addr.split()]\n",
    "common_tokens = Counter(tokens).most_common(100)\n",
    "for token, freq in common_tokens:\n",
    "    if token.isalpha() and freq > 50:\n",
    "        print(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T18:34:20.167549Z",
     "iopub.status.busy": "2025-07-16T18:34:20.167285Z",
     "iopub.status.idle": "2025-07-16T18:34:21.255315Z",
     "shell.execute_reply": "2025-07-16T18:34:21.254616Z",
     "shell.execute_reply.started": "2025-07-16T18:34:20.167530Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', 'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2', 'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n",
      "\n",
      " {0: 'B-APARTMENT', 1: 'B-AREA', 2: 'B-BLOCK', 3: 'B-FLOOR', 4: 'B-HOUSE_NUMBER', 5: 'B-KHASRA_NUMBER', 6: 'B-LANDMARK', 7: 'B-LOCALITY', 8: 'B-LOCALITY2', 9: 'B-PINCODE', 10: 'B-PLOT_NUMBER', 11: 'B-ROAD_DETAILS', 12: 'B-VILLAGE', 13: 'I-APARTMENT', 14: 'I-AREA', 15: 'I-BLOCK', 16: 'I-FLOOR', 17: 'I-HOUSE_NUMBER', 18: 'I-KHASRA_NUMBER', 19: 'I-LANDMARK', 20: 'I-LOCALITY', 21: 'I-LOCALITY2', 22: 'I-PLOT_NUMBER', 23: 'I-ROAD_DETAILS', 24: 'I-VILLAGE', 25: 'O'}\n",
      "\n",
      " {'B-APARTMENT': 0, 'B-AREA': 1, 'B-BLOCK': 2, 'B-FLOOR': 3, 'B-HOUSE_NUMBER': 4, 'B-KHASRA_NUMBER': 5, 'B-LANDMARK': 6, 'B-LOCALITY': 7, 'B-LOCALITY2': 8, 'B-PINCODE': 9, 'B-PLOT_NUMBER': 10, 'B-ROAD_DETAILS': 11, 'B-VILLAGE': 12, 'I-APARTMENT': 13, 'I-AREA': 14, 'I-BLOCK': 15, 'I-FLOOR': 16, 'I-HOUSE_NUMBER': 17, 'I-KHASRA_NUMBER': 18, 'I-LANDMARK': 19, 'I-LOCALITY': 20, 'I-LOCALITY2': 21, 'I-PLOT_NUMBER': 22, 'I-ROAD_DETAILS': 23, 'I-VILLAGE': 24, 'O': 25}\n",
      "\n",
      " 26\n",
      "                                                text  \\\n",
      "0  e - 1 / 1091 , shivaji marg , sonia vihar , de...   \n",
      "\n",
      "                                              labels  \\\n",
      "0  B-HOUSE_NUMBER I-HOUSE_NUMBER I-HOUSE_NUMBER I...   \n",
      "\n",
      "                                           label_ids  \n",
      "0  [4, 17, 17, 17, 17, 25, 11, 23, 25, 7, 20, 25,...  \n"
     ]
    }
   ],
   "source": [
    "# Create label mappings\n",
    "label_list = sorted(set(label for row in df[\"labels\"] for label in row.split()))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "NUM_LABELS = len(label_list)\n",
    "\n",
    "print(\"\\n\",label_list)\n",
    "print(\"\\n\",id2label)\n",
    "print(\"\\n\",label2id)\n",
    "\n",
    "print(\"\\n\",NUM_LABELS)\n",
    "# Map string tags to ID list\n",
    "df[\"label_ids\"] = df[\"labels\"].apply(lambda x: [label2id[l] for l in x.split()])\n",
    "print(df[[\"text\", \"labels\", \"label_ids\"]].head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-07-17T04:07:29.803Z",
     "iopub.execute_input": "2025-07-16T18:34:23.978135Z",
     "iopub.status.busy": "2025-07-16T18:34:23.977397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf5189f40f549b2afe94ae8f992362b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed03bf8cc5e42978d34e19b2b656ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c45a906afd4c81ade8a1026de75644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e47f0b4d45440a8303567193d51511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4d8b983a5a425eb82be5c43deb917b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# üß† Step 2: Tokenizer + Model\n",
    "#model_checkpoint= \"/kaggle/input/muril-results/muril_60k_7epoch\"\n",
    "model_checkpoint = \"google/muril-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(len(tokenizer))\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=NUM_LABELS)  # Set this later\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer,return_tensors=\"pt\")\n",
    "# Sample Hinglish address tokens you‚Äôve seen getting split up\n",
    "new_tokens =[\n",
    "    \"mustafabad\", \"karawal\", \"nagar\", \"sonia\", \"vihar\", \"ngr\", \"fazalpur\",\n",
    "    \"mandawali\", \"marg\", \"shankar\", \"khajoori\", \"khas\", \"gandhi\", \"ganga\",\n",
    "    \"pur\", \"laxmi\", \"madhuban\", \"aruna\", \"shakarpur\", \"johri\", \"nehru\",\n",
    "    \"bhagirathi\", \"dayalpur\",\"cghs\",\"ganj\",\"patpar\",\"pusta\",\"vinod\",\"fazalpur\",\"pandav\"\n",
    "]\n",
    "added = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"‚úÖ Added {added} new tokens to the tokenizer.\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(df[[\"text\", \"labels\", \"label_ids\"]].head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T18:38:08.187444Z",
     "iopub.status.busy": "2025-07-16T18:38:08.186948Z",
     "iopub.status.idle": "2025-07-16T18:39:13.001289Z",
     "shell.execute_reply": "2025-07-16T18:39:13.000530Z",
     "shell.execute_reply.started": "2025-07-16T18:38:08.187422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Rows with token/label mismatch: 3\n",
      "                                                    text  \\\n",
      "4      old no - 271 , new b - 75 , block b , g no - 5...   \n",
      "16     old no b - 215 , new b - 197 / 2 , chandu naga...   \n",
      "66479  uppar bal g - 4 , hn200 , gn4 pusta - 5 sonia ...   \n",
      "\n",
      "                                                  tokens  \\\n",
      "4      [old, no, -, 271, ,, new, b, -, 75, ,, block, ...   \n",
      "16     [old, no, b, -, 215, ,, new, b, -, 197, /, 2, ...   \n",
      "66479  [uppar, bal, g, -, 4, ,, hn200, ,, gn4, pusta,...   \n",
      "\n",
      "                                               label_ids  \n",
      "4      [4, 17, 25, 17, 17, 25, 2, 15, 25, 11, 23, 23,...  \n",
      "16     [4, 17, 17, 25, 17, 17, 17, 17, 25, 7, 20, 25,...  \n",
      "66479  [25, 25, 4, 17, 17, 25, 17, 25, 11, 7, 20, 25, 9]  \n",
      "                                                text  \\\n",
      "0  e - 1 / 1091 , shivaji marg , sonia vihar , de...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [e, -, 1, /, 1091, ,, shivaji, marg, ,, sonia,...   \n",
      "\n",
      "Dataset({\n",
      "    features: ['tokens', 'label_ids', '__index_level_0__'],\n",
      "    num_rows: 120761\n",
      "}) \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f814c49d2e44ea09784565998bc833f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
      "    num_rows: 120761\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# ü™Ñ Step 1: Tokenize text into words\n",
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x))\n",
    "\n",
    "\n",
    "df[\"num_tokens\"] = df[\"tokens\"].apply(len)\n",
    "df[\"num_labels\"] = df[\"label_ids\"].apply(len)\n",
    "mismatched_rows = df[df[\"num_tokens\"] != df[\"num_labels\"]]\n",
    "print(f\"üîç Rows with token/label mismatch: {len(mismatched_rows)}\")\n",
    "print(mismatched_rows[[\"text\", \"tokens\", \"label_ids\"]].head(10))\n",
    "df.drop(index=[4,16,66479], inplace=True)\n",
    "#235572,6098,36417,53991,63119,124552 \n",
    "# üß† Step 2: Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"tokens\",\"label_ids\"]])\n",
    "print(df[[\"text\", \"tokens\"]].head(1),\"\\n\")\n",
    "print(dataset,\"\\n\")\n",
    "\n",
    "# üß† Step 3: Tokenize and align labels properly\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=200\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids()\n",
    "    label_ids = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    label_all_tokens = True  # üëà Enable this at the top\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            current_label = example[\"label_ids\"][word_idx]\n",
    "            if not label_all_tokens and word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(current_label)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    # ‚úÖ Return only what the model expects\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"token_type_ids\": tokenized.get(\"token_type_ids\", [0] * 128),\n",
    "        \"labels\": label_ids\n",
    "    }\n",
    "\n",
    "# üöÄ Step 4: Apply tokenizer + cleanup\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=[\"tokens\", \"label_ids\"]\n",
    ")\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T18:39:18.828541Z",
     "iopub.status.busy": "2025-07-16T18:39:18.828226Z",
     "iopub.status.idle": "2025-07-16T18:39:18.836294Z",
     "shell.execute_reply": "2025-07-16T18:39:18.835484Z",
     "shell.execute_reply.started": "2025-07-16T18:39:18.828513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi‚Äëclass token classification.\n",
    "    Œ≥ (gamma) amplifies the effect on hard examples.\n",
    "    Œ± (alpha) is a per‚Äëclass weight (use your class_weights tensor here).\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, ignore_index=-100, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha          # tensor of shape [num_labels] or None\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: [N, C]  targets: [N]\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits, targets,\n",
    "            weight=self.alpha,\n",
    "            ignore_index=self.ignore_index,\n",
    "            reduction=\"none\"\n",
    "        )\n",
    "        # Only keep valid positions\n",
    "        valid_mask = (targets != self.ignore_index).float()\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        targets_one_hot = F.one_hot(targets.clamp(min=0), num_classes=logits.size(-1)).float()\n",
    "        pt = (probs * targets_one_hot).sum(dim=-1)\n",
    "\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = focal_term * ce_loss * valid_mask\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.sum() / valid_mask.sum()\n",
    "        else:\n",
    "            return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T18:39:23.364947Z",
     "iopub.status.busy": "2025-07-16T18:39:23.364423Z",
     "iopub.status.idle": "2025-07-16T18:40:07.693411Z",
     "shell.execute_reply": "2025-07-16T18:40:07.692819Z",
     "shell.execute_reply.started": "2025-07-16T18:39:23.364924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 96608\n",
      "Eval size: 24153\n",
      "Dataset({\n",
      "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
      "    num_rows: 96608\n",
      "})\n",
      "‚úÖ Clipped class weights:\n",
      "B-APARTMENT    : 0.1912\n",
      "B-AREA         : 0.0647\n",
      "B-BLOCK        : 0.1125\n",
      "B-FLOOR        : 0.0277\n",
      "B-HOUSE_NUMBER : 0.0146\n",
      "B-KHASRA_NUMBER: 0.0221\n",
      "B-LANDMARK     : 0.2524\n",
      "B-LOCALITY     : 0.0147\n",
      "B-LOCALITY2    : 0.1256\n",
      "B-PINCODE      : 0.0076\n",
      "B-PLOT_NUMBER  : 0.1228\n",
      "B-ROAD_DETAILS : 0.0201\n",
      "B-VILLAGE      : 0.4730\n",
      "I-APARTMENT    : 0.1211\n",
      "I-AREA         : 0.0699\n",
      "I-BLOCK        : 0.0718\n",
      "I-FLOOR        : 0.0179\n",
      "I-HOUSE_NUMBER : 0.0047\n",
      "I-KHASRA_NUMBER: 0.0109\n",
      "I-LANDMARK     : 0.1544\n",
      "I-LOCALITY     : 0.0135\n",
      "I-LOCALITY2    : 0.1512\n",
      "I-PLOT_NUMBER  : 0.0348\n",
      "I-ROAD_DETAILS : 0.0055\n",
      "I-VILLAGE      : 1.0000\n",
      "O              : 0.0025\n",
      "‚úÖ Batch 0 loaded\n",
      "‚úÖ Batch 1 loaded\n",
      "‚úÖ Batch 2 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/763231292.py:128: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedNERTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedNERTrainer(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# üß† Step 5: Training Arguments + Trainer\n",
    "from transformers import TrainingArguments,logging\n",
    "import torch\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# üÜï  B‚Äë1: Custom trainer that plugs weighted CE‚Äëloss\n",
    "class WeightedNERTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,**kwargs):\n",
    "        labels  = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.get(\"logits\")\n",
    "        focal_loss_fct = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "        loss = focal_loss_fct(logits.view(-1, logits.size(-1)),\n",
    "                        labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "transformers.logging.set_verbosity_debug()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./muril_ner_logs\",\n",
    "    run_name=\"muril_ner_v1\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=6,                 # ‚úÖ Let trainer handle epochs\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",          # ‚úÖ Log every epoch\n",
    "    eval_strategy=\"epoch\",       # ‚úÖ Evaluate every epoch\n",
    "    save_strategy=\"epoch\",             # ‚úÖ Save every epoch\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",  # üëà cosine schedule\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,#During each optimiser step the parameters are nudged toward zero,Prevents a few neurons from dominating and over‚Äëfitting rare quirks in the data.\n",
    "    load_best_model_at_end=True,       # üëà Optional: restores best eval F1\n",
    "    metric_for_best_model=\"f1\",        # üëà requires compute_metrics\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_labels = [[id2label[label] for label in example if label != -100] for example in labels]\n",
    "    true_preds = [[id2label[pred] for pred, label in zip(example_pred, example_label) if label != -100]\n",
    "                  for example_pred, example_label in zip(preds, labels)]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
    "    }\n",
    "# üî• Correct one-time split\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "print(train_dataset)\n",
    "\n",
    "#calculating class-weights from train set\n",
    "from collections import Counter\n",
    "# üîß NEW: compute class weights with clipping on dominant class (like 'O')\n",
    "from collections import Counter\n",
    "\n",
    "def compute_clipped_class_weights(train_ds,\n",
    "                                  label2id,\n",
    "                                  pad_id=-100,\n",
    "                                  target_max=1.0,\n",
    "                                  eps=1e-9):\n",
    "    \"\"\"\n",
    "    Computes inverse-frequency weights, clipped so dominant tag (like 'O') ‚â§ target_max.\n",
    "    \"\"\"\n",
    "    # Count label frequencies\n",
    "    counts = Counter()\n",
    "    for sample in train_ds:\n",
    "        for lid in sample[\"labels\"]:\n",
    "            if lid != pad_id:\n",
    "                counts[lid] += 1\n",
    "\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    # Inverse frequency weights\n",
    "    raw = {i: 1.0 / (counts.get(i, 1) / total + eps) for i in range(len(label2id))}\n",
    "\n",
    "    # Normalize weights\n",
    "    max_raw = max(raw.values())\n",
    "    norm = {i: w / max_raw for i, w in raw.items()}\n",
    "\n",
    "    # Clip dominant tag weight (usually 'O')\n",
    "    o_id = label2id[\"O\"]\n",
    "    if norm[o_id] > target_max:\n",
    "        norm[o_id] = target_max\n",
    "\n",
    "    # Convert to tensor\n",
    "    weight_tensor = torch.tensor(\n",
    "        [norm.get(i, 1.0) for i in range(len(label2id))]\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    print(\"‚úÖ Clipped class weights:\")\n",
    "    for i in range(len(label2id)):\n",
    "        print(f\"{id2label[i]:15s}: {weight_tensor[i].item():.4f}\")\n",
    "    \n",
    "    return weight_tensor\n",
    "\n",
    "# üí• Generate clipped weights from your actual training set\n",
    "class_weights = compute_clipped_class_weights(train_dataset, label2id, target_max=1.0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# ‚úÖ Basic sanity test: Can we load a few batches without hanging?\n",
    "loader = DataLoader(train_dataset, batch_size=2,collate_fn=data_collator)\n",
    "for i, batch in enumerate(loader):\n",
    "    print(f\"‚úÖ Batch {i} loaded\")\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "# ‚úÖ Pass to trainer\n",
    "trainer = WeightedNERTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    #callbacks=[GradientPrinterCallback()]\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T18:42:14.872003Z",
     "iopub.status.busy": "2025-07-16T18:42:14.871562Z",
     "iopub.status.idle": "2025-07-16T21:26:16.339208Z",
     "shell.execute_reply": "2025-07-16T21:26:16.338563Z",
     "shell.execute_reply.started": "2025-07-16T18:42:14.871978Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 32\n",
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 96,608\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 9,054\n",
      "  Number of trainable parameters = 237,000,218\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7550' max='9054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7550/9054 2:43:58 < 32:40, 0.77 it/s, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.038421</td>\n",
       "      <td>0.761304</td>\n",
       "      <td>0.854306</td>\n",
       "      <td>0.805128</td>\n",
       "      <td>0.844295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.024381</td>\n",
       "      <td>0.760200</td>\n",
       "      <td>0.854029</td>\n",
       "      <td>0.804388</td>\n",
       "      <td>0.850645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>0.769295</td>\n",
       "      <td>0.863440</td>\n",
       "      <td>0.813653</td>\n",
       "      <td>0.854588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.012126</td>\n",
       "      <td>0.763615</td>\n",
       "      <td>0.858752</td>\n",
       "      <td>0.808394</td>\n",
       "      <td>0.853434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.766545</td>\n",
       "      <td>0.860463</td>\n",
       "      <td>0.810793</td>\n",
       "      <td>0.855166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24153\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./muril_ner_logs/checkpoint-1510\n",
      "Configuration saved in ./muril_ner_logs/checkpoint-1510/config.json\n",
      "Model weights saved in ./muril_ner_logs/checkpoint-1510/model.safetensors\n",
      "tokenizer config file saved in ./muril_ner_logs/checkpoint-1510/tokenizer_config.json\n",
      "Special tokens file saved in ./muril_ner_logs/checkpoint-1510/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24153\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./muril_ner_logs/checkpoint-3020\n",
      "Configuration saved in ./muril_ner_logs/checkpoint-3020/config.json\n",
      "Model weights saved in ./muril_ner_logs/checkpoint-3020/model.safetensors\n",
      "tokenizer config file saved in ./muril_ner_logs/checkpoint-3020/tokenizer_config.json\n",
      "Special tokens file saved in ./muril_ner_logs/checkpoint-3020/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24153\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./muril_ner_logs/checkpoint-4530\n",
      "Configuration saved in ./muril_ner_logs/checkpoint-4530/config.json\n",
      "Model weights saved in ./muril_ner_logs/checkpoint-4530/model.safetensors\n",
      "tokenizer config file saved in ./muril_ner_logs/checkpoint-4530/tokenizer_config.json\n",
      "Special tokens file saved in ./muril_ner_logs/checkpoint-4530/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24153\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./muril_ner_logs/checkpoint-6040\n",
      "Configuration saved in ./muril_ner_logs/checkpoint-6040/config.json\n",
      "Model weights saved in ./muril_ner_logs/checkpoint-6040/model.safetensors\n",
      "tokenizer config file saved in ./muril_ner_logs/checkpoint-6040/tokenizer_config.json\n",
      "Special tokens file saved in ./muril_ner_logs/checkpoint-6040/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24153\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./muril_ner_logs/checkpoint-7550\n",
      "Configuration saved in ./muril_ner_logs/checkpoint-7550/config.json\n",
      "Model weights saved in ./muril_ner_logs/checkpoint-7550/model.safetensors\n",
      "tokenizer config file saved in ./muril_ner_logs/checkpoint-7550/tokenizer_config.json\n",
      "Special tokens file saved in ./muril_ner_logs/checkpoint-7550/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./muril_ner_logs/checkpoint-4530 (score: 0.8136532412822055).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7550, training_loss=0.024251962118590903, metrics={'train_runtime': 9840.8798, 'train_samples_per_second': 58.902, 'train_steps_per_second': 0.92, 'total_flos': 4.9314129770112e+16, 'train_loss': 0.024251962118590903, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "#3013 0.97it/s  \n",
    "trainer.train()        \n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T21:45:06.176540Z",
     "iopub.status.busy": "2025-07-16T21:45:06.175910Z",
     "iopub.status.idle": "2025-07-16T21:46:01.133456Z",
     "shell.execute_reply": "2025-07-16T21:46:01.132832Z",
     "shell.execute_reply.started": "2025-07-16T21:45:06.176514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /kaggle/working/muril_ner_final\n",
      "Configuration saved in /kaggle/working/muril_ner_final/config.json\n",
      "Model weights saved in /kaggle/working/muril_ner_final/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/muril_ner_final/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/muril_ner_final/special_tokens_map.json\n",
      "tokenizer config file saved in /kaggle/working/muril_ner_final/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/muril_ner_final/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/muril_ner_final.zip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Save model and tokenizer after training completes\n",
    "model_save_path = \"/kaggle/working/muril_ner_final\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "import shutil\n",
    "shutil.make_archive(\"/kaggle/working/muril_ner_final\", 'zip', model_save_path)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-07-11T10:58:43.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Display a download link in Kaggle\n",
    "FileLink(/kaggle/working/muril_ner_final.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T21:47:28.920702Z",
     "iopub.status.busy": "2025-07-16T21:47:28.920077Z",
     "iopub.status.idle": "2025-07-16T21:47:30.546878Z",
     "shell.execute_reply": "2025-07-16T21:47:30.546063Z",
     "shell.execute_reply.started": "2025-07-16T21:47:28.920677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file /kaggle/working/muril_ner_final/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 197304\n",
      "}\n",
      "\n",
      "loading weights file /kaggle/working/muril_ner_final/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at /kaggle/working/muril_ner_final.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', 'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2', 'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n",
      "\n",
      " {0: 'B-APARTMENT', 1: 'B-AREA', 2: 'B-BLOCK', 3: 'B-FLOOR', 4: 'B-HOUSE_NUMBER', 5: 'B-KHASRA_NUMBER', 6: 'B-LANDMARK', 7: 'B-LOCALITY', 8: 'B-LOCALITY2', 9: 'B-PINCODE', 10: 'B-PLOT_NUMBER', 11: 'B-ROAD_DETAILS', 12: 'B-VILLAGE', 13: 'I-APARTMENT', 14: 'I-AREA', 15: 'I-BLOCK', 16: 'I-FLOOR', 17: 'I-HOUSE_NUMBER', 18: 'I-KHASRA_NUMBER', 19: 'I-LANDMARK', 20: 'I-LOCALITY', 21: 'I-LOCALITY2', 22: 'I-PLOT_NUMBER', 23: 'I-ROAD_DETAILS', 24: 'I-VILLAGE', 25: 'O'}\n",
      "\n",
      " {'B-APARTMENT': 0, 'B-AREA': 1, 'B-BLOCK': 2, 'B-FLOOR': 3, 'B-HOUSE_NUMBER': 4, 'B-KHASRA_NUMBER': 5, 'B-LANDMARK': 6, 'B-LOCALITY': 7, 'B-LOCALITY2': 8, 'B-PINCODE': 9, 'B-PLOT_NUMBER': 10, 'B-ROAD_DETAILS': 11, 'B-VILLAGE': 12, 'I-APARTMENT': 13, 'I-AREA': 14, 'I-BLOCK': 15, 'I-FLOOR': 16, 'I-HOUSE_NUMBER': 17, 'I-KHASRA_NUMBER': 18, 'I-LANDMARK': 19, 'I-LOCALITY': 20, 'I-LOCALITY2': 21, 'I-PLOT_NUMBER': 22, 'I-ROAD_DETAILS': 23, 'I-VILLAGE': 24, 'O': 25}\n",
      "\n",
      " 26\n",
      "                                                text  \\\n",
      "0  mohd shafi qureshi755 gno - 10 rajeev gandhi n...   \n",
      "\n",
      "                                              labels  \\\n",
      "0  B-HOUSE_NUMBER I-HOUSE_NUMBER I-HOUSE_NUMBER B...   \n",
      "\n",
      "                                      label_ids  \n",
      "0  [4, 17, 17, 11, 23, 23, 7, 20, 20, 8, 21, 9]  \n",
      "{0: 'B-APARTMENT', 1: 'B-AREA', 2: 'B-BLOCK', 3: 'B-FLOOR', 4: 'B-HOUSE_NUMBER', 5: 'B-KHASRA_NUMBER', 6: 'B-LANDMARK', 7: 'B-LOCALITY', 8: 'B-LOCALITY2', 9: 'B-PINCODE', 10: 'B-PLOT_NUMBER', 11: 'B-ROAD_DETAILS', 12: 'B-VILLAGE', 13: 'I-APARTMENT', 14: 'I-AREA', 15: 'I-BLOCK', 16: 'I-FLOOR', 17: 'I-HOUSE_NUMBER', 18: 'I-KHASRA_NUMBER', 19: 'I-LANDMARK', 20: 'I-LOCALITY', 21: 'I-LOCALITY2', 22: 'I-PLOT_NUMBER', 23: 'I-ROAD_DETAILS', 24: 'I-VILLAGE', 25: 'O'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "#LOAD EVAL DATASET\n",
    "df = pd.read_csv(\"bio_tagged_data_108361_141435_eval.csv\")  # Make sure labels are space-separated strings\n",
    "df.rename(columns = {'sentence':'text', 'tags':'labels'}, inplace=True)\n",
    "df.describe()\n",
    "\n",
    "# ‚úÖ Load the saved model + tokenizer \n",
    "model_path = \"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\muril_120k_5epochs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "from transformers import pipeline\n",
    "ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",device=0)  # 'simple' groups tokens into entities\n",
    "\n",
    "# Create label mappings\n",
    "label_list = sorted(set(label for row in df[\"labels\"] for label in row.split()))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "NUM_LABELS = len(label_list)\n",
    "\n",
    "print(\"\\n\",label_list)\n",
    "print(\"\\n\",id2label)\n",
    "print(\"\\n\",label2id)\n",
    "\n",
    "print(\"\\n\",NUM_LABELS)\n",
    "# Map string tags to ID list\n",
    "df[\"label_ids\"] = df[\"labels\"].apply(lambda x: [label2id[l] for l in x.split()])\n",
    "print(df[[\"text\", \"labels\", \"label_ids\"]].head(1))\n",
    "df.head()\n",
    "\n",
    "print(id2label)\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = {v: k for k, v in id2label.items()}\n",
    "#final_eval = trainer.evaluate()\n",
    "#print(\"‚úÖ Final Evaluation:\", final_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-16T21:47:51.346985Z",
     "iopub.status.busy": "2025-07-16T21:47:51.346271Z",
     "iopub.status.idle": "2025-07-16T21:47:51.417074Z",
     "shell.execute_reply": "2025-07-16T21:47:51.416348Z",
     "shell.execute_reply.started": "2025-07-16T21:47:51.346957Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'HOUSE_NUMBER', 'score': 0.3710264, 'word': 'House No. 17 - B', 'start': 0, 'end': 14}, {'entity_group': 'PLOT_NUMBER', 'score': 0.32253975, 'word': 'Plot No. 45, Old Plot No. 22', 'start': 16, 'end': 44}, {'entity_group': 'FLOOR', 'score': 0.3685306, 'word': 'Third Floor', 'start': 46, 'end': 57}, {'entity_group': 'ROAD_DETAILS', 'score': 0.35970134, 'word': 'Ga', 'start': 59, 'end': 61}, {'entity_group': 'ROAD_DETAILS', 'score': 0.3228927, 'word': '##li No. 4, Street No. 2, Mohalla Shyam', 'start': 61, 'end': 98}, {'entity_group': 'LOCALITY', 'score': 0.17051268, 'word': 'Nagar', 'start': 99, 'end': 104}, {'entity_group': 'ROAD_DETAILS', 'score': 0.32380474, 'word': ', Main Road', 'start': 104, 'end': 115}, {'entity_group': 'KHASRA_NUMBER', 'score': 0.36270142, 'word': 'K', 'start': 117, 'end': 118}, {'entity_group': 'KHASRA_NUMBER', 'score': 0.36971754, 'word': '##H No. 78 / 3 / 2', 'start': 118, 'end': 130}, {'entity_group': 'BLOCK', 'score': 0.33427504, 'word': 'B Block', 'start': 132, 'end': 139}, {'entity_group': 'APARTMENT', 'score': 0.28512117, 'word': 'Shyam Residency Apartment', 'start': 141, 'end': 166}, {'entity_group': 'LANDMARK', 'score': 0.26788902, 'word': 'Near', 'start': 168, 'end': 172}, {'entity_group': 'LANDMARK', 'score': 0.25545108, 'word': 'Shiv Mandir', 'start': 173, 'end': 184}, {'entity_group': 'LOCALITY', 'score': 0.28409892, 'word': 'Shyam Nagar', 'start': 186, 'end': 197}, {'entity_group': 'LOCALITY2', 'score': 0.15173775, 'word': ',', 'start': 197, 'end': 198}, {'entity_group': 'LOCALITY2', 'score': 0.20509589, 'word': 'Laxmi Nagar,', 'start': 199, 'end': 211}, {'entity_group': 'LOCALITY2', 'score': 0.20902814, 'word': 'Pat', 'start': 212, 'end': 215}, {'entity_group': 'LOCALITY2', 'score': 0.19997674, 'word': '##par ganj Village', 'start': 215, 'end': 230}, {'entity_group': 'PINCODE', 'score': 0.3870275, 'word': '1100', 'start': 240, 'end': 244}, {'entity_group': 'PINCODE', 'score': 0.38704965, 'word': '##92', 'start': 244, 'end': 246}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(ner_pipe(\"House No. 17-B, Plot No. 45, Old Plot No. 22, Third Floor, Gali No. 4, Street No. 2, Mohalla Shyam Nagar, Main Road, KH No. 78/3/2, B Block, Shyam Residency Apartment, Near Shiv Mandir, Shyam Nagar, Laxmi Nagar, Patparganj Village, Delhi - 110092\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-07-17T04:07:29.805Z",
     "iopub.execute_input": "2025-07-16T21:48:05.995723Z",
     "iopub.status.busy": "2025-07-16T21:48:05.995470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Rows with token/label mismatch: 0\n",
      "Empty DataFrame\n",
      "Columns: [text, tokens, label_ids]\n",
      "Index: []\n",
      "                                                text  \\\n",
      "0  mohd shafi qureshi755 gno - 10 rajeev gandhi n...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [mohd, shafi, qureshi755, gno, -, 10, rajeev, ...  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4ae1c2017747a6b32f1b38d48e10a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 33073\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2068' max='2068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2068/2068 03:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    B-APARTMENT       0.45      0.86      0.59      5188\n",
      "         B-AREA       0.44      0.67      0.53     12204\n",
      "        B-BLOCK       0.38      0.83      0.52      3659\n",
      "        B-FLOOR       0.89      0.94      0.91      8459\n",
      " B-HOUSE_NUMBER       0.93      0.94      0.93     35562\n",
      "B-KHASRA_NUMBER       0.75      0.97      0.85     27425\n",
      "     B-LANDMARK       0.36      0.84      0.50      2101\n",
      "     B-LOCALITY       0.69      0.73      0.71     31758\n",
      "    B-LOCALITY2       0.23      0.34      0.28      4435\n",
      "      B-PINCODE       1.00      1.00      1.00     65958\n",
      "  B-PLOT_NUMBER       0.39      0.68      0.49      8848\n",
      " B-ROAD_DETAILS       0.77      0.68      0.72     27923\n",
      "      B-VILLAGE       0.16      0.36      0.22      1236\n",
      "    I-APARTMENT       0.37      0.79      0.50      7392\n",
      "         I-AREA       0.45      0.74      0.56     13469\n",
      "        I-BLOCK       0.35      0.88      0.50      5120\n",
      "        I-FLOOR       0.88      0.91      0.90     13340\n",
      " I-HOUSE_NUMBER       0.94      0.85      0.90    114913\n",
      "I-KHASRA_NUMBER       0.83      0.96      0.89     48686\n",
      "     I-LANDMARK       0.46      0.72      0.56      2863\n",
      "     I-LOCALITY       0.65      0.74      0.69     38589\n",
      "    I-LOCALITY2       0.18      0.35      0.24      4210\n",
      "  I-PLOT_NUMBER       0.50      0.65      0.57     26465\n",
      " I-ROAD_DETAILS       0.84      0.63      0.72     95542\n",
      "      I-VILLAGE       0.10      0.38      0.16       580\n",
      "              O       0.97      0.73      0.83    223736\n",
      "\n",
      "       accuracy                           0.79    829661\n",
      "      macro avg       0.57      0.74      0.63    829661\n",
      "   weighted avg       0.84      0.79      0.80    829661\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#only evaluation \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "# ü™Ñ Step 1: Tokenize text into words\n",
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x))\n",
    "\n",
    "\n",
    "df[\"num_tokens\"] = df[\"tokens\"].apply(len)\n",
    "df[\"num_labels\"] = df[\"label_ids\"].apply(len)\n",
    "mismatched_rows = df[df[\"num_tokens\"] != df[\"num_labels\"]]\n",
    "print(f\"üîç Rows with token/label mismatch: {len(mismatched_rows)}\")\n",
    "print(mismatched_rows[[\"text\", \"tokens\", \"label_ids\"]].head())\n",
    "#df.drop(index=[4, 16], inplace=True)\n",
    "\n",
    "# üß† Step 2: Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"tokens\", \"label_ids\"]])\n",
    "print(df[[\"text\", \"tokens\"]].head(1))\n",
    "\n",
    "# üß† Step 3: Tokenize and align labels properly\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=200\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids()\n",
    "    label_ids = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    label_all_tokens = True  # üëà Enable this at the top\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            current_label = example[\"label_ids\"][word_idx]\n",
    "            if not label_all_tokens and word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(current_label)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    # ‚úÖ Return only what the model expects\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"token_type_ids\": tokenized.get(\"token_type_ids\", [0] * 128),\n",
    "        \"labels\": label_ids\n",
    "    }\n",
    "\n",
    "# üöÄ Step 4: Apply tokenizer + cleanup\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=[\"tokens\", \"label_ids\"]\n",
    ")\n",
    "\n",
    "\n",
    "#seperate trainer for evaluation \n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\", per_device_eval_batch_size=16\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    true = p.label_ids\n",
    "    pred_tags, true_tags = [], []\n",
    "\n",
    "    for pred_seq, true_seq in zip(preds, true):\n",
    "        for p, t in zip(pred_seq, true_seq):\n",
    "            if t != -100:\n",
    "                pred_tags.append(id2label[p])\n",
    "                true_tags.append(id2label[t])\n",
    "\n",
    "    report = classification_report(true_tags, pred_tags, zero_division=0, output_dict=False)\n",
    "    print(report)\n",
    "        # üìâ Confusion matrix\n",
    "    labels = list(id2label.values())  # all label strings in order\n",
    "    cm = confusion_matrix(true_tags, pred_tags, labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(cm,\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"Blues\",\n",
    "                annot=False,\n",
    "                fmt=\"d\",\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"NER Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # üíæ Save plot\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    plt.savefig(\"results/confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    return {}\n",
    "\n",
    "trainer = Trainer(model=model, args=args,compute_metrics=compute_metrics)\n",
    "trainer.evaluate(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-07-17T04:07:29.805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"/kaggle/input/address-ner/bio_tagged_data_108361_141435_eval.csv\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "raw_addresses = df['sentence'].tolist()\n",
    "#print(raw_addresses)\n",
    "print(label_list)\n",
    "\n",
    "# Predict in batches (not all at once)\n",
    "batch_results = []\n",
    "for chunk in tqdm(range(0, len(raw_addresses), 16)):\n",
    "    batch = raw_addresses[chunk:chunk+16]\n",
    "    batch_results.extend(ner_pipe(batch))\n",
    "print('batch results',batch_results)\n",
    "# Then postprocess as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-07-17T04:07:29.805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ‚úèÔ∏è 1) Edit this list if you add / drop labels later\n",
    "FIELDS = [\n",
    "    \"HOUSE_NUMBER\", \"PLOT_NUMBER\", \"FLOOR\", \"ROAD_DETAILS\", \"KHASRA_NUMBER\",\n",
    "    \"BLOCK\", \"APARTMENT\", \"LANDMARK\", \"LOCALITY\", \"AREA\", \"LOCALITY2\",\n",
    "    \"VILLAGE\", \"PINCODE\"\n",
    "]\n",
    "\n",
    "# üîß 2) Utility to stitch sub‚Äëwords: \"##no\" ‚Üí \"no\" (attach to previous token)\n",
    "def join_word(prev, cur):\n",
    "    if cur.startswith(\"##\"):\n",
    "        return prev + cur[2:]        # no space, drop ##\n",
    "    if prev.endswith(\"-\") or prev.endswith(\"/\"):\n",
    "        return prev + cur            # e.g. \"23/\" + \"A\"\n",
    "    if prev:                         # normal case: add a space\n",
    "        return prev + \" \" + cur\n",
    "    return cur                       # first token\n",
    "\n",
    "# üèóÔ∏è 3) Convert one NER output list ‚Üí dict of fields\n",
    "def convert_single_output(entity_list):\n",
    "    row = defaultdict(str)\n",
    "\n",
    "    for ent in entity_list:\n",
    "        key  = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"]\n",
    "        if row[key]:\n",
    "            row[key] = join_word(row[key], word)\n",
    "        else:\n",
    "            row[key] = word.lstrip(\"##\")  # first token\n",
    "\n",
    "    # üåê Compose complete address (simple comma join)\n",
    "    ordered_parts = [row[f] for f in FIELDS if row[f]]\n",
    "    if row[\"PINCODE\"]:\n",
    "        ordered_parts.append(f\"Delhi -{row['PINCODE']}\")   \n",
    "    row[\"complete_address\"] = \", \".join(ordered_parts)\n",
    "\n",
    "    return row\n",
    "\n",
    "# üöÄ 4) Run on your whole batch_results list\n",
    "parsed_rows = [convert_single_output(addr_entities) for addr_entities in batch_results]\n",
    "df = pd.DataFrame(parsed_rows)\n",
    "\n",
    "# üîñ 5) Re‚Äëorder columns and save\n",
    "df = df[FIELDS + [\"complete_address\"]]\n",
    "df.to_csv(\"parsed_address.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved parsed_address.csv\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-07-13T12:50:04.438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Predict NER on each address\n",
    "predicted = []\n",
    "for address in raw_addresses:\n",
    "    ents = ner_pipe(address)\n",
    "    predicted.append(ents)\n",
    "\n",
    "# Let's look!\n",
    "for i, address in enumerate(raw_addresses):\n",
    "    print(f\"\\nüì¨ {address}\")\n",
    "    for ent in predicted[i]:\n",
    "        print(f\"{ent['word']:<20} ‚ûú {ent['entity_group']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-09T09:27:35.437614Z",
     "iopub.status.idle": "2025-07-09T09:27:35.438193Z",
     "shell.execute_reply": "2025-07-09T09:27:35.438036Z",
     "shell.execute_reply.started": "2025-07-09T09:27:35.438020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "structured_data = []\n",
    "\n",
    "for ents in predicted:\n",
    "    entity_map = defaultdict(str)\n",
    "    for ent in ents:\n",
    "        label = ent[\"entity_group\"].lower().replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        entity_map[label] += ent[\"word\"] + \" \"\n",
    "    structured_data.append({k.strip(): v.strip() for k, v in entity_map.items()})\n",
    "\n",
    "df_structured = pd.DataFrame(structured_data)\n",
    "print(df_structured.describe(),\"\\n\")\n",
    "print(df_structured.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7840587,
     "sourceId": 12440652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7652307,
     "sourceId": 12489222,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "grok_address_parser-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
