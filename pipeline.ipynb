{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b23c4",
   "metadata": {},
   "source": [
    "PART 1:NER INFERENCE PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\address_cluster_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.54.0\n",
      "CUDA Available? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "#!pip install -q datasets seqeval\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re   \n",
    "\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "print(\"CUDA Available?\", torch.cuda.is_available())\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Load your dataset\n",
    "main_df=pd.read_csv(\"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\exp1.csv\")#path to your dataset\n",
    "main_df['CUSTADDR'] = main_df['CUSTADDR'].str.lower()\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13732ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', 'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2', 'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n",
      "\n",
      " {0: 'B-APARTMENT', 1: 'B-AREA', 2: 'B-BLOCK', 3: 'B-FLOOR', 4: 'B-HOUSE_NUMBER', 5: 'B-KHASRA_NUMBER', 6: 'B-LANDMARK', 7: 'B-LOCALITY', 8: 'B-LOCALITY2', 9: 'B-PINCODE', 10: 'B-PLOT_NUMBER', 11: 'B-ROAD_DETAILS', 12: 'B-VILLAGE', 13: 'I-APARTMENT', 14: 'I-AREA', 15: 'I-BLOCK', 16: 'I-FLOOR', 17: 'I-HOUSE_NUMBER', 18: 'I-KHASRA_NUMBER', 19: 'I-LANDMARK', 20: 'I-LOCALITY', 21: 'I-LOCALITY2', 22: 'I-PLOT_NUMBER', 23: 'I-ROAD_DETAILS', 24: 'I-VILLAGE', 25: 'O'}\n",
      "\n",
      " {'B-APARTMENT': 0, 'B-AREA': 1, 'B-BLOCK': 2, 'B-FLOOR': 3, 'B-HOUSE_NUMBER': 4, 'B-KHASRA_NUMBER': 5, 'B-LANDMARK': 6, 'B-LOCALITY': 7, 'B-LOCALITY2': 8, 'B-PINCODE': 9, 'B-PLOT_NUMBER': 10, 'B-ROAD_DETAILS': 11, 'B-VILLAGE': 12, 'I-APARTMENT': 13, 'I-AREA': 14, 'I-BLOCK': 15, 'I-FLOOR': 16, 'I-HOUSE_NUMBER': 17, 'I-KHASRA_NUMBER': 18, 'I-LANDMARK': 19, 'I-LOCALITY': 20, 'I-LOCALITY2': 21, 'I-PLOT_NUMBER': 22, 'I-ROAD_DETAILS': 23, 'I-VILLAGE': 24, 'O': 25}\n",
      "\n",
      " 26\n",
      "{0: 'B-APARTMENT', 1: 'B-AREA', 2: 'B-BLOCK', 3: 'B-FLOOR', 4: 'B-HOUSE_NUMBER', 5: 'B-KHASRA_NUMBER', 6: 'B-LANDMARK', 7: 'B-LOCALITY', 8: 'B-LOCALITY2', 9: 'B-PINCODE', 10: 'B-PLOT_NUMBER', 11: 'B-ROAD_DETAILS', 12: 'B-VILLAGE', 13: 'I-APARTMENT', 14: 'I-AREA', 15: 'I-BLOCK', 16: 'I-FLOOR', 17: 'I-HOUSE_NUMBER', 18: 'I-KHASRA_NUMBER', 19: 'I-LANDMARK', 20: 'I-LOCALITY', 21: 'I-LOCALITY2', 22: 'I-PLOT_NUMBER', 23: 'I-ROAD_DETAILS', 24: 'I-VILLAGE', 25: 'O'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "# ‚úÖ Load the saved model + tokenizer \n",
    "model_path = \"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\muril_120k_5epochs\"#path to ner model folder\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "from transformers import pipeline\n",
    "ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",device=0)  # 'simple' groups tokens into entities\n",
    "\n",
    "# Create label mappings\n",
    "label_list1=['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', \n",
    "             'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2',\n",
    "             'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n",
    "#label_list = sorted(set(label for row in df[\"labels\"] for label in row.split()))\n",
    "label2id = {label: i for i, label in enumerate(label_list1)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "NUM_LABELS = len(label_list1)\n",
    "\n",
    "print(\"\\n\",label_list1)\n",
    "print(\"\\n\",id2label)\n",
    "print(\"\\n\",label2id)\n",
    "\n",
    "print(\"\\n\",NUM_LABELS)\n",
    "# Map string tags to ID list\n",
    "#df[\"label_ids\"] = df[\"labels\"].apply(lambda x: [label2id[l] for l in x.split()])\n",
    "#print(df[[\"text\", \"labels\", \"label_ids\"]].head(1))\n",
    "#df.head()\n",
    "\n",
    "print(id2label)\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = {v: k for k, v in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8253f2e",
   "metadata": {},
   "source": [
    "NEXT CELL DOES BASIC NORMALIZATION ON ADDRESSES AND SAVE IT IN A NEW COLOMN 'ADDR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:128: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:128: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Windows\\Temp\\ipykernel_17196\\4055270440.py:128: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  main_df=pd.read_csv(\"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\exp1.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID Division                                           CUSTADDR\n",
      "0   CA1       D1             j 3 118 j laxmi nagar extension 110092\n",
      "1   CA2       D1             j 3 163 laxmi nagar j extension 110092\n",
      "2   CA3       D1             f 86 a jawahar park laxmi nagar 110092\n",
      "3   CA4       D1                           i 186 laxmi nagar 110092\n",
      "4   CA5       D1  house number 5 195 lalita park laxmi nagar 110092\n",
      "5   CA6       D1                            i 34 laxmi nagar 110092\n",
      "6   CA7       D1           10 a vishwakarma park laxmi nagar 110092\n",
      "7   CA8       D1       18 1 g f vishwakarma park laxmi nagar 110092\n",
      "8   CA9       D1            7 a vishwakarma park laxmi nagar 110092\n",
      "9  CA10       D1                         i 173 a laxmi nagar 110092\n",
      "      ID Division                                  CUSTADDR\n",
      "73  CA74       D2  harijan basti tukmir pur shahdara 110094\n",
      "74  CA75       D2           vill tukmir pur shahdara 110094\n",
      "87  CA88       D2        vill khajoori khas shahdara 110094\n",
      "88  CA89       D2                e 4 494 sonia vihar 110094\n",
      "89  CA90       D2          e 20 khajoori khas colony 110094\n",
      "90  CA91       D2        vill khajoori khas shahdara 110094\n",
      "91  CA92       D2                 vill khajoori khas 110094\n",
      "92  CA93       D2  f 587 khajoori khas karawal nagar 110094\n",
      "93  CA94       D2                    tukmir pur vill 110094\n",
      "94  CA95       D2                   22 khajoori khas 110094\n",
      "            ID Division                        CUSTADDR\n",
      "count   261778   261778                          261778\n",
      "unique  261778        1                          256743\n",
      "top       CA74       D2  vill johri pur shahdara 110094\n",
      "freq         1   261778                              96\n",
      "            ID Division                   CUSTADDR\n",
      "count   215078   215078                     215078\n",
      "unique  215078        1                     202505\n",
      "top        CA1       D1  mandawali fazalpur 110092\n",
      "freq         1   215078                        114\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def standardize_address_abbreviations_enhanced(address_text):\n",
    "    \"\"\"\n",
    "    Standardizes common abbreviations in an an address string, with enhanced\n",
    "    handling for house numbers where abbreviations might be merged with digits.\n",
    "    \"\"\"\n",
    "    if not isinstance(address_text, str):\n",
    "        return address_text # Return as is if not a string (e.g., None, NaN)\n",
    "\n",
    "    address_text = address_text.lower() # Convert to lowercase for consistent matching\n",
    "\n",
    "    # --- NEW Step 0: General Cleaning and Space Introduction ---\n",
    "\n",
    "    # 0.1: Replace hyphens and slashes with spaces\n",
    "    address_text = re.sub(r'[-/]', ' ', address_text)\n",
    "\n",
    "    # 0.2: Introduce a space when switching from an alphabet to a number (e.g., \"A123\" -> \"A 123\")\n",
    "    address_text = re.sub(r'([a-z])(\\d)', r'\\1 \\2', address_text)\n",
    "\n",
    "    # 0.3: Introduce a space when switching from a number to an alphabet (e.g., \"123A\" -> \"123 A\")\n",
    "    address_text = re.sub(r'(\\d)([a-z])', r'\\1 \\2', address_text)\n",
    "\n",
    "    # 0.4: Normalize multiple spaces to a single space, and strip leading/trailing spaces\n",
    "    address_text = re.sub(r'\\s+', ' ', address_text).strip()\n",
    "\n",
    "    # --- Previous Step 1: Pre-process specific merged abbreviations (now less critical but still useful) ---\n",
    "    # These specific regex patterns might still catch nuanced cases not covered by generic alpha-numeric split.\n",
    "    # House Number patterns\n",
    "    address_text = re.sub(r'\\b(h|house|hn|hno|no|prop no|flat no|unit no|bldg no|room no|ro)\\s*(\\d+[a-z]*)\\b', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(no|h no|h\\.no|h-no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(house no|hse no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(prop no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(bldg no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(flat no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(room no|ro)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "\n",
    "\n",
    "    # Gali Number patterns\n",
    "    address_text = re.sub(r'\\b(g|gali|gali no|g no)\\s*(\\d+[a-z]*)\\b', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(gali no|g no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(gali|g)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "\n",
    "    # --- Step 2: Define and apply comprehensive word-boundary based replacements ---\n",
    "    FUZZY_MAP = {\n",
    "        r'\\bhn\\b': 'house number',\n",
    "        r'\\bh no\\b': 'house number',\n",
    "        r'\\bhouse no\\b': 'house number',\n",
    "        r'\\bhse no\\b': 'house number',\n",
    "        r'\\bhs no\\b': 'house number',\n",
    "        r'\\bhno\\b': 'house number',\n",
    "        r'\\bprop no\\b': 'property number',\n",
    "        r'\\bflat no\\b': 'flat number',\n",
    "        r'\\bunit no\\b': 'unit number',\n",
    "        r'\\bkh no\\b': 'khasra number',\n",
    "        r'\\bkhasra no\\b': 'khasra number',\n",
    "        r'\\bkhno\\b': 'khasra number',\n",
    "        r'\\bkhn\\b': 'khasra number',\n",
    "\n",
    "\n",
    "        r'\\bg no\\b': 'gali number',\n",
    "        r'\\bgali no\\b': 'gali number',\n",
    "        r'\\bgno\\b': 'gali number',\n",
    "        r'\\bgn\\b': 'gali number',\n",
    "\n",
    "        r'\\bpno\\b': 'plot number',\n",
    "\n",
    "        r'\\bextn\\b': 'extension',\n",
    "        r'\\bext\\b': 'extension',\n",
    "        r'\\bextnti\\b': 'extension',\n",
    "        \n",
    "\n",
    "        r'\\bngr\\b': 'nagar',\n",
    "        r'\\bk ngr\\b': 'karawal nagar',\n",
    "        r'\\blakshmi\\b': 'laxmi',\n",
    "        r'\\bvill\\b': 'village',\n",
    "\n",
    "\n",
    "        r'\\bph\\b': 'phase',\n",
    "        r'\\bphs\\b': 'phase',\n",
    "        r'\\bphase no\\b': 'phase number',\n",
    "\n",
    "       \n",
    "        r'\\brm\\b': 'room',\n",
    "        r'\\broom no\\b': 'room number',\n",
    "\n",
    "\n",
    "        r'\\brod\\b': 'road',\n",
    "        r'\\bmarg\\b': 'road',\n",
    "       \n",
    "\n",
    "        r'\\bldg\\b': 'building',\n",
    "        r'\\bbldg no\\b': 'building number',\n",
    "        r'\\bblk\\b': 'block',\n",
    "        r'\\bbl\\b': 'block',\n",
    "\n",
    "        r'\\bapp\\b': 'apartment',\n",
    "        r'\\bapt\\b': 'apartment',\n",
    "        r'\\bfl\\b': 'floor',\n",
    "        r'\\bg f\\b': 'ground floor',\n",
    "        r'\\bf f\\b': 'first floor',\n",
    "        r'\\bii f\\b': 'second floor',\n",
    "        r'\\biii f\\b': 'third floor',\n",
    "\n",
    "        r'\\blc\\b': 'locality',\n",
    "        r'\\barea\\b': 'area',\n",
    "        r'\\bpincode\\b': 'pincode',\n",
    "        r'\\bpin code\\b': 'pincode',\n",
    "\n",
    "        r'\\bnr\\b': 'near',\n",
    "        r'\\bopp\\b': 'opposite',\n",
    "        r'\\bop\\b': 'opposite',\n",
    "        r'\\b(no)\\b': 'number', # General 'no' at the end, if not caught by specific patterns\n",
    "    }    # Sort keys by length (descending) to ensure longer, more specific matches are attempted first.\n",
    "    sorted_fuzzy_map_keys = sorted(FUZZY_MAP.keys(), key=len, reverse=True)\n",
    "\n",
    "    for abbr_pattern in sorted_fuzzy_map_keys:\n",
    "        full_form = FUZZY_MAP[abbr_pattern]\n",
    "        address_text = re.sub(abbr_pattern, full_form, address_text)\n",
    "\n",
    "    # Clean up multiple spaces that might result from replacements\n",
    "    address_text = re.sub(r'\\s+', ' ', address_text).strip()\n",
    "    return address_text\n",
    "\n",
    "import pandas as pd\n",
    "# Separate data by division\n",
    "df_d1 = main_df[main_df['Division'] == 'D1'].copy()\n",
    "df_d2 = main_df[main_df['Division'] == 'D2'].copy()\n",
    "\n",
    "df_d1['ADDR'] = df_d1['CUSTADDR'].apply(standardize_address_abbreviations_enhanced)\n",
    "df_d2['ADDR'] = df_d2['CUSTADDR'].apply(standardize_address_abbreviations_enhanced)\n",
    "print(df_d1.head(10))\n",
    "print(df_d2.head(10))\n",
    "print(df_d2.describe())\n",
    "print(df_d1.describe())\n",
    "#df_standardized.to_csv(\"standardized_address_regex_based2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1667e9",
   "metadata": {},
   "source": [
    "THIS CELL IS THE MAIN NER MODEL INFERENCE CELL , WE GET RAW OUTPUTS WITH CONFIDENCE SCORE ON EACH TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3322948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', 'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2', 'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3361/3361 [1:38:40<00:00,  1.76s/it]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "print(label_list1)\n",
    "\n",
    "def ner_batch_runner(df):\n",
    "    # Prepare HuggingFace Dataset\n",
    "    dataset = Dataset.from_pandas(df[['ID', 'ADDR','CUSTADDR']])\n",
    "\n",
    "    # Batch inference\n",
    "    def run_batch(batch):\n",
    "        predictions = ner_pipe(batch['ADDR'])\n",
    "        return {\"entities\": predictions}\n",
    "\n",
    "    # Run with batched=True\n",
    "    dataset = dataset.map(run_batch, batched=True, batch_size=128)\n",
    "\n",
    "    # Convert back to DataFrame with tqdm progress\n",
    "    entity_df = dataset.to_pandas()\n",
    "    return entity_df\n",
    "\n",
    "# === Run on both D1 and D2 ===\n",
    "print(\"Running NER on D1...\")\n",
    "ner_df1 = ner_batch_runner(df_d1)\n",
    "print(ner_df1.head())\n",
    "print(ner_df1.describe())\n",
    "print(\"Running NER on D2...\")\n",
    "ner_df2 = ner_batch_runner(df_d2)\n",
    "print(ner_df1.head())\n",
    "print(ner_df1.describe())\n",
    "# Optional: Save to file\n",
    "ner_df1.to_csv(\"d1_ner_output.csv\", index=False)\n",
    "ner_df2.to_csv(\"d2_ner_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4fc8c",
   "metadata": {},
   "source": [
    "THIS CELL PARSES RAW NER OUTPUT INTO COLOMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db11374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added parsed addresses to structured CSVs.\n",
      "    ID    HOUSE_NUMBER PLOT_NUMBER FLOOR ROAD_DETAILS KHASRA_NUMBER BLOCK  \\\n",
      "0  CA1             j 3       118 j                                          \n",
      "1  CA2         j 3 163                          laxmi                       \n",
      "2  CA3          f 86 a                        jawahar                       \n",
      "3  CA4           i 186                                                      \n",
      "4  CA5  house number 5         195                                          \n",
      "\n",
      "  APARTMENT LANDMARK     LOCALITY               AREA LOCALITY2 VILLAGE  \\\n",
      "0                           nagar    laxmi extension                     \n",
      "1                                  nagar j extension                     \n",
      "2                            park        laxmi nagar                     \n",
      "3                     laxmi nagar                                        \n",
      "4                     lalita park        laxmi nagar                     \n",
      "\n",
      "  PINCODE                                   complete_address  \\\n",
      "0  110092         j 3, 118 j, nagar, laxmi extension, 110092   \n",
      "1  110092          j 3 163, laxmi, nagar j extension, 110092   \n",
      "2  110092         f 86 a, jawahar, park, laxmi nagar, 110092   \n",
      "3  110092                         i 186, laxmi nagar, 110092   \n",
      "4  110092  house number 5, 195, lalita park, laxmi nagar,...   \n",
      "\n",
      "                                                ADDR  \n",
      "0             j 3 118 j laxmi nagar extension 110092  \n",
      "1             j 3 163 laxmi nagar j extension 110092  \n",
      "2             f 86 a jawahar park laxmi nagar 110092  \n",
      "3                           i 186 laxmi nagar 110092  \n",
      "4  house number 5 195 lalita park laxmi nagar 110092  \n",
      "     ID HOUSE_NUMBER PLOT_NUMBER FLOOR     ROAD_DETAILS KHASRA_NUMBER BLOCK  \\\n",
      "0  CA74                                                                       \n",
      "1  CA75                                                                       \n",
      "2  CA88                                                                       \n",
      "3  CA89            e       4 494                                              \n",
      "4  CA90         e 20                    khajoori colony                       \n",
      "\n",
      "  APARTMENT LANDMARK       LOCALITY AREA            LOCALITY2  \\\n",
      "0                     harijan basti       tukmir pur shahdara   \n",
      "1                                                    shahdara   \n",
      "2                                                    shahdara   \n",
      "3                       sonia vihar                             \n",
      "4                              khas                             \n",
      "\n",
      "                 VILLAGE PINCODE                            complete_address  \\\n",
      "0                         110094  harijan basti, tukmir pur shahdara, 110094   \n",
      "1     village tukmir pur  110094        shahdara, village tukmir pur, 110094   \n",
      "2  village khajoori khas  110094     shahdara, village khajoori khas, 110094   \n",
      "3                         110094               e, 4 494, sonia vihar, 110094   \n",
      "4                         110094         e 20, khajoori colony, khas, 110094   \n",
      "\n",
      "                                       ADDR  \n",
      "0  harijan basti tukmir pur shahdara 110094  \n",
      "1        village tukmir pur shahdara 110094  \n",
      "2     village khajoori khas shahdara 110094  \n",
      "3                e 4 494 sonia vihar 110094  \n",
      "4          e 20 khajoori khas colony 110094  \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# ‚úèÔ∏è 1) Edit this list if you add / drop labels later\n",
    "FIELDS = [\n",
    "    \"HOUSE_NUMBER\", \"PLOT_NUMBER\", \"FLOOR\", \"ROAD_DETAILS\", \"KHASRA_NUMBER\",\n",
    "    \"BLOCK\", \"APARTMENT\", \"LANDMARK\", \"LOCALITY\", \"AREA\", \"LOCALITY2\",\n",
    "    \"VILLAGE\", \"PINCODE\"\n",
    "]\n",
    "\n",
    "# üîß 2) Utility to stitch sub-words: \"##no\" ‚Üí \"no\" (attach to previous token)\n",
    "def join_word(prev, cur):\n",
    "    if cur.startswith(\"##\"):\n",
    "        return prev + cur[2:]\n",
    "    if prev.endswith(\"-\") or prev.endswith(\"/\"):\n",
    "        return prev + cur\n",
    "    return prev + \" \" + cur if prev else cur\n",
    "\n",
    "# üèóÔ∏è 3) Convert one NER output list ‚Üí dict of fields\n",
    "def convert_single_output(entity_list):\n",
    "    row = defaultdict(str)\n",
    "\n",
    "    for ent in entity_list:\n",
    "        key = ent[\"entity_group\"].upper()\n",
    "        word = ent[\"word\"]\n",
    "        if row[key]:\n",
    "            row[key] = join_word(row[key], word)\n",
    "        else:\n",
    "            row[key] = word.lstrip(\"##\")\n",
    "\n",
    "    # üåê Compose complete address (simple comma join)\n",
    "    ordered_parts = [row[f] for f in FIELDS if row[f]]\n",
    "    #if row[\"PINCODE\"]:\n",
    "    #    ordered_parts.append(f\"Delhi -{row['PINCODE']}\")\n",
    "    row[\"complete_address\"] = \", \".join(ordered_parts)\n",
    "\n",
    "    return row\n",
    "\n",
    "# üöÄ 4) Parse both NER outputs into structured format\n",
    "def parse_ner_output(ner_df):\n",
    "    parsed_rows = [convert_single_output(ents) for ents in ner_df[\"entities\"]]\n",
    "    parsed_df = pd.DataFrame(parsed_rows)\n",
    "    return pd.concat([ner_df[[\"ID\",\"ADDR\"]].reset_index(drop=True), parsed_df], axis=1)\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def fix_entity_column(df):\n",
    "    def fix_json_str(s):\n",
    "        if isinstance(s, str):\n",
    "            # Add missing commas between dictionaries\n",
    "            s = re.sub(r\"\\}\\s*\\{\", \"}, {\", s)\n",
    "            try:\n",
    "                return ast.literal_eval(s)\n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è Failed to parse:\", s)\n",
    "                return []\n",
    "        return s\n",
    "    df[\"entities\"] = df[\"entities\"].apply(fix_json_str)\n",
    "    return df\n",
    "\n",
    "#temprory\n",
    "ner_df1=pd.read_csv(\"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\d1_ner_output.csv\")\n",
    "ner_df2=pd.read_csv(\"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\d2_ner_output.csv\")\n",
    "ner_df1 = fix_entity_column(ner_df1)\n",
    "ner_df2 = fix_entity_column(ner_df2)\n",
    "# üóÇÔ∏è 5) Process D1 and D2\n",
    "df1 = parse_ner_output(ner_df1)\n",
    "df2 = parse_ner_output(ner_df2)\n",
    "\n",
    "# üìù 6) Save parsed addresses\n",
    "df1 = df1[[\"ID\"] + FIELDS + [\"complete_address\"]+[\"CUSTADDR\"]]\n",
    "df2 = df2[[\"ID\"] + FIELDS + [\"complete_address\"]+[\"CUSTADDR\"]]\n",
    "\n",
    "df1.to_csv(\"d1_structured_parsed.csv\", index=False)\n",
    "df2.to_csv(\"d2_structured_parsed.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Added parsed addresses to structured CSVs.\")\n",
    "print(df1.head())\n",
    "print(df2.head()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b7bf6",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "CLUSTERING USING EMBEDDINGS ON EXTRACTED SENTENCE using paraphrase-multilingual-MiniLM-L12-v2 AND FAISS \n",
    "---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece73609",
   "metadata": {},
   "source": [
    "Global HNSW indexing(graph based clustering technique),here we make indexes on the complete vector db so can be ram intensive on very huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4861a8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\address_cluster_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MODEL_NAME           = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "EMBED_CACHE_DIR      = \"embeddings_cache\"\n",
    "GLOBAL_THRESHOLD     = 0.97\n",
    "GLOBAL_EFSEARCH      = 32#recall\n",
    "HNSW_M               = 32#neighbouring nodes\n",
    "HNSW_EFCONSTRUCT     = 80#accuracy and buld time\n",
    "\n",
    "STOP_WORDS           = ['house number', 'plot number']\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "\n",
    "def embed_and_cache(df, col=\"address_extract\", cache_key=\"global\"):\n",
    "    os.makedirs(EMBED_CACHE_DIR, exist_ok=True)\n",
    "    path = f\"{EMBED_CACHE_DIR}/{cache_key}.npy\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"üìÇ Loading embeddings from cache: {path}\")\n",
    "        return np.load(path)\n",
    "    print(\"üß† Computing embeddings‚Ä¶\")\n",
    "    model = SentenceTransformer(MODEL_NAME,device=device)\n",
    "    embs = model.encode(df[col].tolist(), show_progress_bar=True, normalize_embeddings=True)\n",
    "    np.save(path, embs)\n",
    "    return embs\n",
    "\n",
    "\n",
    "def build_hnsw(embeddings, M=HNSW_M, efC=HNSW_EFCONSTRUCT):\n",
    "    dim = embeddings.shape[1]\n",
    "    idx = faiss.IndexHNSWFlat(dim, M)\n",
    "    idx.hnsw.efConstruction = efC\n",
    "    idx.add(embeddings)  # embeddings are already normalized\n",
    "    return idx\n",
    "\n",
    "\n",
    "def global_cluster(df, embeddings, threshold=GLOBAL_THRESHOLD, ef_search=GLOBAL_EFSEARCH):\n",
    "    ids  = df[\"ID\"].tolist()\n",
    "    addr = df[\"address_extract\"].tolist()\n",
    "    raw  = df[\"CUSTADDR\"].tolist()\n",
    "\n",
    "    index = build_hnsw(embeddings)\n",
    "    index.hnsw.efSearch = ef_search\n",
    "\n",
    "    k = min(100, len(addr))\n",
    "    _, neighbors = index.search(embeddings, k)\n",
    "\n",
    "    visited = set()\n",
    "    clusters = []\n",
    "    sim_band_counter = Counter()\n",
    "\n",
    "    print(\"üîó Clustering similar addresses globally...\")\n",
    "    for i, nbrs in tqdm(enumerate(neighbors), total=len(neighbors), desc=\"Global Clustering\"):\n",
    "        if ids[i] in visited:\n",
    "            continue\n",
    "\n",
    "        cluster_idxs = []\n",
    "        for j in nbrs:\n",
    "            if j == -1:\n",
    "                continue\n",
    "            sim = float(np.dot(embeddings[i], embeddings[j]))\n",
    "            band = round(sim, 2)\n",
    "            sim_band_counter[band] += 1\n",
    "\n",
    "            if sim >= threshold and ids[j] not in visited:\n",
    "                visited.add(ids[j])\n",
    "                cluster_idxs.append(j)\n",
    "\n",
    "        if len(cluster_idxs) > 1:\n",
    "            clusters.append({\n",
    "                \"cluster_id\": f\"cluster_{i}\",\n",
    "                \"canonical_address\": addr[i],\n",
    "                \"members\": [\n",
    "                    {\"customer_id\": ids[j], \"raw_address\": raw[j]}\n",
    "                    for j in cluster_idxs\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    return clusters, sim_band_counter\n",
    "\n",
    "\n",
    "def mini_cluster_house_plot(clusters, df):\n",
    "    df_meta = df.set_index(\"ID\")\n",
    "    enhanced = []\n",
    "\n",
    "    print(\"üè† Creating mini-clusters based on HOUSE_NUMBER and PLOT_NUMBER...\")\n",
    "    for cl in tqdm(clusters, desc=\"Mini Clustering\", total=len(clusters)):\n",
    "        buckets = defaultdict(list)\n",
    "\n",
    "        for m in cl[\"members\"]:\n",
    "            cid = m[\"customer_id\"]\n",
    "            try:\n",
    "                h = str(df_meta.at[cid, \"HOUSE_NUMBER\"]).strip().lower()\n",
    "                p = str(df_meta.at[cid, \"PLOT_NUMBER\"]).strip().lower()\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # Clean STOP_WORDS (if any)\n",
    "            for w in STOP_WORDS:\n",
    "                h = h.replace(w, \"\")\n",
    "                p = p.replace(w, \"\")\n",
    "            h, p = h.strip(), p.strip()\n",
    "\n",
    "            # üõë Skip empty both\n",
    "            if not h and not p:\n",
    "                continue\n",
    "\n",
    "            key = (h, p)\n",
    "            buckets[key].append(m)\n",
    "\n",
    "        subcls = []\n",
    "        for (h, p), membs in buckets.items():\n",
    "            if len(membs) < 2:\n",
    "                continue\n",
    "            subcls.append({\n",
    "                \"subcluster_id\": f\"{cl['cluster_id']}_h{h or 'na'}_p{p or 'na'}\",\n",
    "                \"house_number\": h or None,\n",
    "                \"plot_number\": p or None,\n",
    "                \"members\": membs\n",
    "            })\n",
    "\n",
    "        cl[\"mini_clusters\"] = subcls\n",
    "        enhanced.append(cl)\n",
    "\n",
    "    return enhanced\n",
    "\n",
    "\n",
    "def report_stats(clusters, sim_counter):\n",
    "    sizes = [len(c[\"members\"]) for c in clusters]\n",
    "    return {\n",
    "        \"total_clusters\": len(clusters),\n",
    "        \"avg_cluster_size\": float(np.mean(sizes)) if sizes else 0,\n",
    "        \"max_cluster_size\": int(max(sizes)) if sizes else 0,\n",
    "        \"cluster_size_dist\": dict(Counter(sizes)),\n",
    "        \"similarity_band_dist\": dict(sim_counter)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_24028\\2393011998.py:1: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2= pd.read_csv('D:/ML/grok_adddress_parser/grok_address_parser-env/d2_structured_parsed.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading embeddings from cache: embeddings_cache/all_addresses_in_d2.npy\n",
      "üîó Clustering similar addresses globally...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global Clustering: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 261778/261778 [00:29<00:00, 8815.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè† Creating mini-clusters based on HOUSE_NUMBER and PLOT_NUMBER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mini Clustering: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35596/35596 [00:01<00:00, 31816.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done for d2. Summary: {\n",
      "  \"total_clusters\": 35596,\n",
      "  \"avg_cluster_size\": 3.562787953702663,\n",
      "  \"max_cluster_size\": 100,\n",
      "  \"cluster_size_dist\": {\n",
      "    \"2\": 20548,\n",
      "    \"27\": 21,\n",
      "    \"21\": 43,\n",
      "    \"22\": 27,\n",
      "    \"3\": 6704,\n",
      "    \"6\": 915,\n",
      "    \"5\": 1637,\n",
      "    \"7\": 621,\n",
      "    \"33\": 8,\n",
      "    \"11\": 170,\n",
      "    \"20\": 37,\n",
      "    \"4\": 2983,\n",
      "    \"37\": 7,\n",
      "    \"29\": 16,\n",
      "    \"100\": 11,\n",
      "    \"8\": 388,\n",
      "    \"9\": 307,\n",
      "    \"12\": 144,\n",
      "    \"13\": 145,\n",
      "    \"18\": 40,\n",
      "    \"96\": 3,\n",
      "    \"25\": 15,\n",
      "    \"36\": 8,\n",
      "    \"10\": 248,\n",
      "    \"16\": 64,\n",
      "    \"15\": 77,\n",
      "    \"31\": 8,\n",
      "    \"51\": 2,\n",
      "    \"77\": 1,\n",
      "    \"49\": 2,\n",
      "    \"14\": 95,\n",
      "    \"24\": 17,\n",
      "    \"70\": 2,\n",
      "    \"56\": 5,\n",
      "    \"19\": 35,\n",
      "    \"26\": 12,\n",
      "    \"43\": 8,\n",
      "    \"23\": 19,\n",
      "    \"17\": 47,\n",
      "    \"34\": 8,\n",
      "    \"95\": 1,\n",
      "    \"28\": 11,\n",
      "    \"39\": 5,\n",
      "    \"98\": 3,\n",
      "    \"55\": 3,\n",
      "    \"50\": 7,\n",
      "    \"67\": 2,\n",
      "    \"38\": 8,\n",
      "    \"99\": 2,\n",
      "    \"35\": 14,\n",
      "    \"44\": 5,\n",
      "    \"32\": 10,\n",
      "    \"83\": 2,\n",
      "    \"69\": 1,\n",
      "    \"59\": 3,\n",
      "    \"62\": 2,\n",
      "    \"97\": 1,\n",
      "    \"46\": 5,\n",
      "    \"68\": 4,\n",
      "    \"30\": 10,\n",
      "    \"94\": 1,\n",
      "    \"47\": 4,\n",
      "    \"48\": 1,\n",
      "    \"75\": 1,\n",
      "    \"66\": 2,\n",
      "    \"64\": 1,\n",
      "    \"54\": 1,\n",
      "    \"42\": 2,\n",
      "    \"73\": 3,\n",
      "    \"40\": 4,\n",
      "    \"74\": 1,\n",
      "    \"65\": 6,\n",
      "    \"91\": 1,\n",
      "    \"61\": 2,\n",
      "    \"80\": 1,\n",
      "    \"93\": 2,\n",
      "    \"63\": 2,\n",
      "    \"60\": 1,\n",
      "    \"58\": 2,\n",
      "    \"87\": 2,\n",
      "    \"41\": 3,\n",
      "    \"71\": 1,\n",
      "    \"57\": 2,\n",
      "    \"52\": 1,\n",
      "    \"82\": 1,\n",
      "    \"89\": 1\n",
      "  },\n",
      "  \"similarity_band_dist\": {\n",
      "    \"1.0\": 185692,\n",
      "    \"0.95\": 984348,\n",
      "    \"0.94\": 1387797,\n",
      "    \"0.92\": 1833268,\n",
      "    \"0.91\": 1865615,\n",
      "    \"0.9\": 1740997,\n",
      "    \"0.89\": 1494591,\n",
      "    \"0.88\": 1214314,\n",
      "    \"0.87\": 958017,\n",
      "    \"0.86\": 732844,\n",
      "    \"0.97\": 231402,\n",
      "    \"0.93\": 1670649,\n",
      "    \"0.96\": 556233,\n",
      "    \"0.85\": 554633,\n",
      "    \"0.84\": 413205,\n",
      "    \"0.83\": 300513,\n",
      "    \"0.82\": 221442,\n",
      "    \"0.81\": 161351,\n",
      "    \"0.8\": 119223,\n",
      "    \"0.79\": 86973,\n",
      "    \"0.78\": 60783,\n",
      "    \"0.77\": 44272,\n",
      "    \"0.76\": 33314,\n",
      "    \"0.75\": 24009,\n",
      "    \"0.74\": 18819,\n",
      "    \"0.99\": 20488,\n",
      "    \"0.98\": 74758,\n",
      "    \"0.73\": 13885,\n",
      "    \"0.72\": 10236,\n",
      "    \"0.71\": 7392,\n",
      "    \"0.7\": 6149,\n",
      "    \"0.69\": 4931,\n",
      "    \"0.68\": 3524,\n",
      "    \"0.67\": 3526,\n",
      "    \"0.66\": 2581,\n",
      "    \"0.63\": 1523,\n",
      "    \"0.62\": 1063,\n",
      "    \"0.61\": 1059,\n",
      "    \"0.6\": 923,\n",
      "    \"0.59\": 602,\n",
      "    \"0.58\": 536,\n",
      "    \"0.65\": 2265,\n",
      "    \"0.64\": 1665,\n",
      "    \"0.56\": 391,\n",
      "    \"0.55\": 291,\n",
      "    \"0.54\": 325,\n",
      "    \"0.53\": 543,\n",
      "    \"0.52\": 150,\n",
      "    \"0.51\": 305,\n",
      "    \"0.57\": 566,\n",
      "    \"0.5\": 119,\n",
      "    \"0.49\": 216,\n",
      "    \"0.48\": 128,\n",
      "    \"0.47\": 76,\n",
      "    \"0.46\": 49,\n",
      "    \"0.45\": 52,\n",
      "    \"0.44\": 78,\n",
      "    \"0.43\": 29,\n",
      "    \"0.42\": 29,\n",
      "    \"0.41\": 48,\n",
      "    \"0.4\": 85,\n",
      "    \"0.39\": 12,\n",
      "    \"0.38\": 6,\n",
      "    \"0.37\": 18,\n",
      "    \"0.36\": 74\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "df2= pd.read_csv('D:/ML/grok_adddress_parser/grok_address_parser-env/d2_structured_parsed.csv')\n",
    "\n",
    "columns_to_merge = [\n",
    "    'HOUSE_NUMBER', 'PLOT_NUMBER', 'ROAD_DETAILS', 'LOCALITY',\n",
    "    'AREA', 'LOCALITY2', 'VILLAGE','APARTMENT'\n",
    "]\n",
    "\n",
    "# Combine fields into a structured string\n",
    "df2['address_extract'] = df2[columns_to_merge].fillna('').apply(lambda row: ', '.join(filter(None, row)), axis=1)\n",
    "df2['address_extract'] = df2['address_extract'].str.replace(r'\\bdelhi\\b', '', case=False, regex=True).str.strip(', ').str.strip()\n",
    "# Fill missing locality values for grouping\n",
    "df2['LOCCALITY'] = df2['LOCALITY'].fillna('unknown').str.lower().str.strip()\n",
    "df2['ID'] = df2['ID'].astype(str)\n",
    "df2[\"HOUSE_NUMBER\"] = df2[\"HOUSE_NUMBER\"].fillna(\"\").astype(str)\n",
    "df2[\"PLOT_NUMBER\"] = df2[\"PLOT_NUMBER\"].fillna(\"\").astype(str)\n",
    "\n",
    "#d2_clustering steps\n",
    "raw_addresses = df2[\"CUSTADDR\"].tolist()\n",
    "customer_ids  = df2[\"ID\"].tolist()\n",
    "\n",
    "embs = embed_and_cache(df2, col=\"address_extract\", cache_key=\"all_addresses_in_d2\")\n",
    "top_clusters,sim_counter = global_cluster(df2, embs, threshold=0.97, ef_search=128)\n",
    "final = mini_cluster_house_plot(top_clusters, df2)\n",
    "stats = report_stats(final,sim_counter)\n",
    "out = {\"summary_stats\": stats, \"clusters\": final}\n",
    "\n",
    "os.makedirs(\"cluster_output\", exist_ok=True)\n",
    "with open(\"cluster_output/complete_d2.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Done for d2. Summary:\", json.dumps(stats, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2735b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Computing embeddings‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/6722 [00:00<?, ?it/s]d:\\conda_envs\\address_cluster_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6722/6722 [01:18<00:00, 85.26it/s] \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "columns_to_merge = [\n",
    "    'HOUSE_NUMBER', 'PLOT_NUMBER', 'ROAD_DETAILS', 'LOCALITY',\n",
    "    'AREA', 'LOCALITY2', 'VILLAGE','APARTMENT'\n",
    "]\n",
    "#d1 clustering steps\n",
    "df1= pd.read_csv('D:/ML/grok_adddress_parser/grok_address_parser-env/d1_structured_parsed.csv')\n",
    "df1['address_extract'] = df1[columns_to_merge].fillna('').apply(lambda row: ', '.join(filter(None, row)), axis=1)\n",
    "df1['address_extract'] = df1['address_extract'].str.replace(r'\\bdelhi\\b', '', case=False, regex=True).str.strip(', ').str.strip()\n",
    "df1['LOCALITY'] = df1['LOCALITY'].fillna('unknown').str.lower().str.strip()\n",
    "df1['ID'] = df1['ID'].astype(str)\n",
    "df1[\"HOUSE_NUMBER\"] = df1[\"HOUSE_NUMBER\"].fillna(\"\").astype(str)\n",
    "df1[\"PLOT_NUMBER\"] = df1[\"PLOT_NUMBER\"].fillna(\"\").astype(str)\n",
    "\n",
    "raw_addresses = df1[\"CUSTADDR\"].tolist()\n",
    "customer_ids  = df1[\"ID\"].tolist()\n",
    "\n",
    "embs = embed_and_cache(df1, col=\"address_extract\", cache_key=\"all_addresses_in_d1\")\n",
    "top_clusters,sim_counter = global_cluster(df1, embs, threshold=0.97, ef_search=128)\n",
    "final = mini_cluster_house_plot(top_clusters, df1)\n",
    "stats = report_stats(final,sim_counter)\n",
    "out = {\"summary_stats\": stats, \"clusters\": final}\n",
    "\n",
    "os.makedirs(\"cluster_output\", exist_ok=True)\n",
    "with open(\"cluster_output/complete_d1.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Done for d1. Summary:\", json.dumps(stats, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89715902",
   "metadata": {},
   "source": [
    "PERFORMED SAME INDEXING TECHNIQUE BUT IN BATCHES AND LATER MERGED DIFFERENT CLUSTERS ACROSS BATCHES BASED ON CENTROID SIMILARITY OF THOSE CLUSTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d85da",
   "metadata": {},
   "source": [
    "CURRENTLY HAS SOME ISSUES WITH RAM USTILIZATION OR DEPENDENCY ISSUES CLUSTERING ALWAYS GETS STUCK AT 35% FOR DATASETS >200K ADDRESSES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc40e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\address_cluster_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MODEL_NAME           = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "EMBED_CACHE_DIR      = \"embeddings_cache\"\n",
    "GLOBAL_THRESHOLD     = 0.97\n",
    "GLOBAL_EFSEARCH      = 20#recall\n",
    "HNSW_M               = 32#neighbouring nodes\n",
    "HNSW_EFCONSTRUCT     = 70#accuracy\n",
    "BATCH_SIZE           = 1000\n",
    "STOP_WORDS           = ['house number', 'plot number']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ FUNCTIONS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def embed_and_cache_batchwise(df, col=\"address_extract\", cache_key=\"global\"):\n",
    "    os.makedirs(EMBED_CACHE_DIR, exist_ok=True)\n",
    "    full_embs = []\n",
    "    model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "    path = f\"{EMBED_CACHE_DIR}/{cache_key}.npy\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"üìÇ Loading embeddings from cache: {path}\")\n",
    "        return np.load(path)\n",
    "    print(\"üß† Computing embeddings in batches‚Ä¶\")\n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "        batch_texts = df[col].iloc[i:i+BATCH_SIZE].tolist()\n",
    "        embs = model.encode(batch_texts, normalize_embeddings=True)\n",
    "        full_embs.append(embs)\n",
    "    full_embs = np.vstack(full_embs)\n",
    "    np.save(path, full_embs)\n",
    "    return full_embs\n",
    "\n",
    "def build_hnsw(embeddings, M=HNSW_M, efC=HNSW_EFCONSTRUCT):\n",
    "    dim = embeddings.shape[1]\n",
    "    idx = faiss.IndexHNSWFlat(dim, M)\n",
    "    idx.hnsw.efConstruction = efC\n",
    "    idx.add(embeddings)\n",
    "    return idx\n",
    "\n",
    "def cluster_batchwise(df, embs, threshold=GLOBAL_THRESHOLD, ef_search=GLOBAL_EFSEARCH):\n",
    "    all_clusters = []\n",
    "    all_centroids = []\n",
    "    id_to_cluster = {}\n",
    "    id_list = df[\"ID\"].tolist()\n",
    "    raw_list = df[\"CUSTADDR\"].tolist()\n",
    "    addr_list = df[\"address_extract\"].tolist()\n",
    "    sim_counter = Counter()\n",
    "\n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Batch Clustering\"):\n",
    "        batch_df = df.iloc[i:i+BATCH_SIZE].copy()\n",
    "        batch_embs = embs[i:i+BATCH_SIZE]\n",
    "        batch_ids = batch_df[\"ID\"].tolist()\n",
    "        batch_raw = batch_df[\"CUSTADDR\"].tolist()\n",
    "        batch_addr = batch_df[\"address_extract\"].tolist()\n",
    "\n",
    "        index = build_hnsw(batch_embs)\n",
    "        index.hnsw.efSearch = ef_search\n",
    "        k = min(100, len(batch_df))\n",
    "        _, neighbors = index.search(batch_embs, k)\n",
    "\n",
    "        visited = set()\n",
    "        for j, nbrs in enumerate(neighbors):\n",
    "            if batch_ids[j] in visited:\n",
    "                continue\n",
    "            cluster_idxs = []\n",
    "            for n in nbrs:\n",
    "                if n == -1:\n",
    "                    continue\n",
    "                sim = float(np.dot(batch_embs[j], batch_embs[n]))\n",
    "                sim_counter[round(sim, 2)] += 1\n",
    "                if sim >= threshold and batch_ids[n] not in visited:\n",
    "                    visited.add(batch_ids[n])\n",
    "                    cluster_idxs.append(n)\n",
    "            if len(cluster_idxs) > 1:\n",
    "                cid = f\"b{i}_cluster_{j}\"\n",
    "                all_clusters.append({\n",
    "                    \"cluster_id\": cid,\n",
    "                    \"canonical_address\": batch_addr[j],\n",
    "                    \"members\": [{\"customer_id\": batch_ids[k], \"raw_address\": batch_raw[k]} for k in cluster_idxs]\n",
    "                })\n",
    "                centroid = np.mean(batch_embs[cluster_idxs], axis=0)\n",
    "                all_centroids.append(centroid)\n",
    "                for k in cluster_idxs:\n",
    "                    id_to_cluster[batch_ids[k]] = cid\n",
    "\n",
    "    return all_clusters, np.vstack(all_centroids), sim_counter\n",
    "\n",
    "def merge_clusters_by_centroids(clusters, centroids, threshold=GLOBAL_THRESHOLD):\n",
    "    nn = NearestNeighbors(n_neighbors=10, metric='cosine').fit(centroids)\n",
    "    distances, indices = nn.kneighbors(centroids)\n",
    "    G = nx.Graph()\n",
    "    for i, nbrs in enumerate(indices):\n",
    "        for j, dist in zip(nbrs, distances[i]):\n",
    "            if dist < (1 - threshold):\n",
    "                G.add_edge(i, j)\n",
    "    components = list(nx.connected_components(G))\n",
    "    merged = []\n",
    "    for idx, comp in enumerate(components):\n",
    "        members = []\n",
    "        for cid in comp:\n",
    "            members.extend(clusters[cid][\"members\"])\n",
    "        if len(members) < 2:\n",
    "            continue\n",
    "        merged.append({\n",
    "            \"cluster_id\": f\"merged_{idx}\",\n",
    "            \"canonical_address\": members[0]['raw_address'],\n",
    "            \"members\": members\n",
    "        })\n",
    "    return merged\n",
    "\n",
    "def mini_cluster_house_plot(clusters, df):\n",
    "    df_meta = df.set_index(\"ID\")\n",
    "    enhanced = []\n",
    "    for cl in tqdm(clusters, desc=\"Mini Clustering\", total=len(clusters)):\n",
    "        buckets = defaultdict(list)\n",
    "        for m in cl[\"members\"]:\n",
    "            cid = m[\"customer_id\"]\n",
    "            try:\n",
    "                h = str(df_meta.at[cid, \"HOUSE_NUMBER\"]).strip().lower()\n",
    "                p = str(df_meta.at[cid, \"PLOT_NUMBER\"]).strip().lower()\n",
    "            except KeyError:\n",
    "                continue\n",
    "            for w in STOP_WORDS:\n",
    "                h = h.replace(w, \"\")\n",
    "                p = p.replace(w, \"\")\n",
    "            if not h and not p:\n",
    "                continue\n",
    "            buckets[(h, p)].append(m)\n",
    "        subcls = []\n",
    "        for (h, p), membs in buckets.items():\n",
    "            if len(membs) < 2:\n",
    "                continue\n",
    "            subcls.append({\n",
    "                \"subcluster_id\": f\"{cl['cluster_id']}_h{h or 'na'}_p{p or 'na'}\",\n",
    "                \"house_number\": h or None,\n",
    "                \"plot_number\": p or None,\n",
    "                \"members\": membs\n",
    "            })\n",
    "        cl[\"mini_clusters\"] = subcls\n",
    "        enhanced.append(cl)\n",
    "    return enhanced\n",
    "\n",
    "def report_stats(clusters, sim_counter):\n",
    "    sizes = [len(c[\"members\"]) for c in clusters]\n",
    "    return {\n",
    "        \"total_clusters\": len(clusters),\n",
    "        \"avg_cluster_size\": float(np.mean(sizes)) if sizes else 0,\n",
    "        \"max_cluster_size\": int(max(sizes)) if sizes else 0,\n",
    "        \"cluster_size_dist\": dict(Counter(sizes)),\n",
    "        \"similarity_band_dist\": dict(sim_counter)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebcb226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_18676\\1818119027.py:1: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2= pd.read_csv('D:/ML/grok_adddress_parser/grok_address_parser-env/d2_structured_parsed.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading embeddings from cache: embeddings_cache/global_d2.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Clustering:   7%|‚ñã         | 18/262 [00:02<00:37,  6.47it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df2= pd.read_csv('D:/ML/grok_adddress_parser/grok_address_parser-env/d2_structured_parsed.csv')\n",
    "\n",
    "columns_to_merge = [\n",
    "    'HOUSE_NUMBER', 'PLOT_NUMBER', 'ROAD_DETAILS', 'LOCALITY',\n",
    "    'AREA', 'LOCALITY2', 'VILLAGE','APARTMENT'\n",
    "]\n",
    "\n",
    "# Combine fields into a structured string\n",
    "df2['address_extract'] = df2[columns_to_merge].fillna('').apply(lambda row: ', '.join(filter(None, row)), axis=1)\n",
    "df2['address_extract'] = df2['address_extract'].str.replace(r'\\bdelhi\\b', '', case=False, regex=True).str.strip(', ').str.strip()\n",
    "# Fill missing locality values for grouping\n",
    "df2['LOCCALITY'] = df2['LOCALITY'].fillna('unknown').str.lower().str.strip()\n",
    "df2['ID'] = df2['ID'].astype(str)\n",
    "df2[\"HOUSE_NUMBER\"] = df2[\"HOUSE_NUMBER\"].fillna(\"\").astype(str)\n",
    "df2[\"PLOT_NUMBER\"] = df2[\"PLOT_NUMBER\"].fillna(\"\").astype(str)\n",
    "\n",
    "#d2_clustering steps\n",
    "raw_addresses = df2[\"CUSTADDR\"].tolist()\n",
    "customer_ids  = df2[\"ID\"].tolist()\n",
    "# ... your preprocessing steps here ...\n",
    "embeddings = embed_and_cache_batchwise(df2, col=\"address_extract\", cache_key=\"global_d2\")\n",
    "clusters, centroids, sim_counter = cluster_batchwise(df2, embeddings)\n",
    "print(f\"üîó Batchwise clusters formed: {len(clusters)}\")\n",
    "\n",
    "# Merge clusters via centroid similarity\n",
    "merged_clusters = merge_clusters_by_centroids(clusters, centroids)\n",
    "print(f\"üß¨ Merged clusters: {len(merged_clusters)}\")\n",
    "\n",
    "    # Sub-cluster using house & plot number\n",
    "final_clusters = mini_cluster_house_plot(merged_clusters, df2)\n",
    "\n",
    "    # Report stats\n",
    "stats = report_stats(final_clusters, sim_counter)\n",
    "print(\"\\nüìä Clustering Summary:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save result\n",
    "with open(\"cluster_output/complete_d2.json\",\"w\") as f:\n",
    "      json.dump({\"summary_stats\": stats, \"clusters\": final_clusters}, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\n‚úÖ Clusters saved to complete_d2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58272176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading embeddings from cache: embeddings_cache/global_d1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Clustering:  14%|‚ñà‚ñç        | 30/216 [00:03<00:24,  7.51it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "columns_to_merge = [\n",
    "    'HOUSE_NUMBER', 'PLOT_NUMBER', 'ROAD_DETAILS', 'LOCALITY',\n",
    "    'AREA', 'LOCALITY2', 'VILLAGE','APARTMENT'\n",
    "]\n",
    "#d1 clustering steps\n",
    "df1= pd.read_csv('D:/ML/grok_adddress_parser/grok_address_parser-env/d1_structured_parsed.csv')\n",
    "df1['address_extract'] = df1[columns_to_merge].fillna('').apply(lambda row: ', '.join(filter(None, row)), axis=1)\n",
    "df1['address_extract'] = df1['address_extract'].str.replace(r'\\bdelhi\\b', '', case=False, regex=True).str.strip(', ').str.strip()\n",
    "df1['LOCALITY'] = df1['LOCALITY'].fillna('unknown').str.lower().str.strip()\n",
    "df1['ID'] = df1['ID'].astype(str)\n",
    "df1[\"HOUSE_NUMBER\"] = df1[\"HOUSE_NUMBER\"].fillna(\"\").astype(str)\n",
    "df1[\"PLOT_NUMBER\"] = df1[\"PLOT_NUMBER\"].fillna(\"\").astype(str)\n",
    "\n",
    "raw_addresses = df1[\"CUSTADDR\"].tolist()\n",
    "customer_ids  = df1[\"ID\"].tolist()\n",
    "embeddings = embed_and_cache_batchwise(df1, col=\"address_extract\", cache_key=\"global_d1\")\n",
    "clusters, centroids, sim_counter = cluster_batchwise(df1, embeddings)\n",
    "print(f\"üîó Batchwise clusters formed: {len(clusters)}\")\n",
    "\n",
    "# Merge clusters via centroid similarity\n",
    "merged_clusters = merge_clusters_by_centroids(clusters, centroids)\n",
    "print(f\"üß¨ Merged clusters: {len(merged_clusters)}\")\n",
    "\n",
    "    # Sub-cluster using house & plot number\n",
    "final_clusters = mini_cluster_house_plot(merged_clusters, df1)\n",
    "\n",
    "    # Report stats\n",
    "stats = report_stats(final_clusters, sim_counter)\n",
    "print(\"\\nüìä Clustering Summary:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save result\n",
    "with open(\"cluster_output/complete_d1.json\",\"w\") as f:\n",
    "      json.dump({\"summary_stats\": stats, \"clusters\": final_clusters}, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\n‚úÖ Clusters saved to complete_d1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5513ee3",
   "metadata": {},
   "source": [
    "BETTER APPROACH IN FUTURE IS TO USE IVF INDEXING TECHNIQUE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d909ff0",
   "metadata": {},
   "source": [
    "and probaly perform clustering in a seperate python script as running notebooks adds extra ram and computation overhead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
