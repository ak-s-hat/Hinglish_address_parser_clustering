{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096b3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.56.0\n",
      "CUDA Available? False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "#!pip install -q datasets seqeval\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "print(\"CUDA Available?\", torch.cuda.is_available())\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ✅ Load your data\n",
    "import pandas as pd\n",
    "#LOAD EVAL DATASET\n",
    "#df = pd.read_csv(\"D:/ML/grok_adddress_parser/grok_address_parser-env/bio_tagged_data_108361_141435_eval.csv\")  # Make sure labels are space-separated strings\n",
    "#df.rename(columns = {'sentence':'text', 'tags':'labels'}, inplace=True)\n",
    "#df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13732ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Windows\\Temp\\ipykernel_55436\\3890445171.py:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  model_path = \"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\muril_120k_5epochs\"\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', 'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2', 'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n",
      "\n",
      " {0: 'B-APARTMENT', 1: 'B-AREA', 2: 'B-BLOCK', 3: 'B-FLOOR', 4: 'B-HOUSE_NUMBER', 5: 'B-KHASRA_NUMBER', 6: 'B-LANDMARK', 7: 'B-LOCALITY', 8: 'B-LOCALITY2', 9: 'B-PINCODE', 10: 'B-PLOT_NUMBER', 11: 'B-ROAD_DETAILS', 12: 'B-VILLAGE', 13: 'I-APARTMENT', 14: 'I-AREA', 15: 'I-BLOCK', 16: 'I-FLOOR', 17: 'I-HOUSE_NUMBER', 18: 'I-KHASRA_NUMBER', 19: 'I-LANDMARK', 20: 'I-LOCALITY', 21: 'I-LOCALITY2', 22: 'I-PLOT_NUMBER', 23: 'I-ROAD_DETAILS', 24: 'I-VILLAGE', 25: 'O'}\n",
      "\n",
      " {'B-APARTMENT': 0, 'B-AREA': 1, 'B-BLOCK': 2, 'B-FLOOR': 3, 'B-HOUSE_NUMBER': 4, 'B-KHASRA_NUMBER': 5, 'B-LANDMARK': 6, 'B-LOCALITY': 7, 'B-LOCALITY2': 8, 'B-PINCODE': 9, 'B-PLOT_NUMBER': 10, 'B-ROAD_DETAILS': 11, 'B-VILLAGE': 12, 'I-APARTMENT': 13, 'I-AREA': 14, 'I-BLOCK': 15, 'I-FLOOR': 16, 'I-HOUSE_NUMBER': 17, 'I-KHASRA_NUMBER': 18, 'I-LANDMARK': 19, 'I-LOCALITY': 20, 'I-LOCALITY2': 21, 'I-PLOT_NUMBER': 22, 'I-ROAD_DETAILS': 23, 'I-VILLAGE': 24, 'O': 25}\n",
      "\n",
      " 26\n",
      "{0: 'B-APARTMENT', 1: 'B-AREA', 2: 'B-BLOCK', 3: 'B-FLOOR', 4: 'B-HOUSE_NUMBER', 5: 'B-KHASRA_NUMBER', 6: 'B-LANDMARK', 7: 'B-LOCALITY', 8: 'B-LOCALITY2', 9: 'B-PINCODE', 10: 'B-PLOT_NUMBER', 11: 'B-ROAD_DETAILS', 12: 'B-VILLAGE', 13: 'I-APARTMENT', 14: 'I-AREA', 15: 'I-BLOCK', 16: 'I-FLOOR', 17: 'I-HOUSE_NUMBER', 18: 'I-KHASRA_NUMBER', 19: 'I-LANDMARK', 20: 'I-LOCALITY', 21: 'I-LOCALITY2', 22: 'I-PLOT_NUMBER', 23: 'I-ROAD_DETAILS', 24: 'I-VILLAGE', 25: 'O'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "# ✅ Load the saved model + tokenizer \n",
    "model_path = \"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\muril_120k_5epochs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "from transformers import pipeline\n",
    "ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",device=0)  # 'simple' groups tokens into entities\n",
    "\n",
    "# Create label mappings\n",
    "label_list1=['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', \n",
    "             'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2',\n",
    "             'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n",
    "#label_list = sorted(set(label for row in df[\"labels\"] for label in row.split()))\n",
    "label2id = {label: i for i, label in enumerate(label_list1)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "NUM_LABELS = len(label_list1)\n",
    "\n",
    "print(\"\\n\",label_list1)\n",
    "print(\"\\n\",id2label)\n",
    "print(\"\\n\",label2id)\n",
    "\n",
    "print(\"\\n\",NUM_LABELS)\n",
    "# Map string tags to ID list\n",
    "#df[\"label_ids\"] = df[\"labels\"].apply(lambda x: [label2id[l] for l in x.split()])\n",
    "#print(df[[\"text\", \"labels\", \"label_ids\"]].head(1))\n",
    "#df.head()\n",
    "\n",
    "print(id2label)\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = {v: k for k, v in id2label.items()}\n",
    "#final_eval = trainer.evaluate()\n",
    "#print(\"✅ Final Evaluation:\", final_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:128: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:128: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Windows\\Temp\\ipykernel_55436\\4055270440.py:128: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  main_df=pd.read_csv(\"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\exp1.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID Division                                           CUSTADDR\n",
      "0   CA1       D1             j 3 118 j laxmi nagar extension 110092\n",
      "1   CA2       D1             j 3 163 laxmi nagar j extension 110092\n",
      "2   CA3       D1             f 86 a jawahar park laxmi nagar 110092\n",
      "3   CA4       D1                           i 186 laxmi nagar 110092\n",
      "4   CA5       D1  house number 5 195 lalita park laxmi nagar 110092\n",
      "5   CA6       D1                            i 34 laxmi nagar 110092\n",
      "6   CA7       D1           10 a vishwakarma park laxmi nagar 110092\n",
      "7   CA8       D1       18 1 g f vishwakarma park laxmi nagar 110092\n",
      "8   CA9       D1            7 a vishwakarma park laxmi nagar 110092\n",
      "9  CA10       D1                         i 173 a laxmi nagar 110092\n",
      "      ID Division                                  CUSTADDR\n",
      "73  CA74       D2  harijan basti tukmir pur shahdara 110094\n",
      "74  CA75       D2           vill tukmir pur shahdara 110094\n",
      "87  CA88       D2        vill khajoori khas shahdara 110094\n",
      "88  CA89       D2                e 4 494 sonia vihar 110094\n",
      "89  CA90       D2          e 20 khajoori khas colony 110094\n",
      "90  CA91       D2        vill khajoori khas shahdara 110094\n",
      "91  CA92       D2                 vill khajoori khas 110094\n",
      "92  CA93       D2  f 587 khajoori khas karawal nagar 110094\n",
      "93  CA94       D2                    tukmir pur vill 110094\n",
      "94  CA95       D2                   22 khajoori khas 110094\n",
      "            ID Division                        CUSTADDR\n",
      "count   261778   261778                          261778\n",
      "unique  261778        1                          256743\n",
      "top       CA74       D2  vill johri pur shahdara 110094\n",
      "freq         1   261778                              96\n",
      "            ID Division                   CUSTADDR\n",
      "count   215078   215078                     215078\n",
      "unique  215078        1                     202505\n",
      "top        CA1       D1  mandawali fazalpur 110092\n",
      "freq         1   215078                        114\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def standardize_address_abbreviations_enhanced(address_text):\n",
    "    \"\"\"\n",
    "    Standardizes common abbreviations in an an address string, with enhanced\n",
    "    handling for house numbers where abbreviations might be merged with digits.\n",
    "    \"\"\"\n",
    "    if not isinstance(address_text, str):\n",
    "        return address_text # Return as is if not a string (e.g., None, NaN)\n",
    "\n",
    "    address_text = address_text.lower() # Convert to lowercase for consistent matching\n",
    "\n",
    "    # --- NEW Step 0: General Cleaning and Space Introduction ---\n",
    "\n",
    "    # 0.1: Replace hyphens and slashes with spaces\n",
    "    address_text = re.sub(r'[-/]', ' ', address_text)\n",
    "\n",
    "    # 0.2: Introduce a space when switching from an alphabet to a number (e.g., \"A123\" -> \"A 123\")\n",
    "    address_text = re.sub(r'([a-z])(\\d)', r'\\1 \\2', address_text)\n",
    "\n",
    "    # 0.3: Introduce a space when switching from a number to an alphabet (e.g., \"123A\" -> \"123 A\")\n",
    "    address_text = re.sub(r'(\\d)([a-z])', r'\\1 \\2', address_text)\n",
    "\n",
    "    # 0.4: Normalize multiple spaces to a single space, and strip leading/trailing spaces\n",
    "    address_text = re.sub(r'\\s+', ' ', address_text).strip()\n",
    "\n",
    "    # --- Previous Step 1: Pre-process specific merged abbreviations (now less critical but still useful) ---\n",
    "    # These specific regex patterns might still catch nuanced cases not covered by generic alpha-numeric split.\n",
    "    # House Number patterns\n",
    "    address_text = re.sub(r'\\b(h|house|hn|hno|no|prop no|flat no|unit no|bldg no|room no|ro)\\s*(\\d+[a-z]*)\\b', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(no|h no|h\\.no|h-no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(house no|hse no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(prop no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(bldg no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(flat no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(room no|ro)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "\n",
    "\n",
    "    # Gali Number patterns\n",
    "    address_text = re.sub(r'\\b(g|gali|gali no|g no)\\s*(\\d+[a-z]*)\\b', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(gali no|g no)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "    address_text = re.sub(r'(gali|g)\\s*(\\d+([a-z])?)', r'\\1 \\2', address_text)\n",
    "\n",
    "    # --- Step 2: Define and apply comprehensive word-boundary based replacements ---\n",
    "    FUZZY_MAP = {\n",
    "        r'\\bhn\\b': 'house number',\n",
    "        r'\\bh no\\b': 'house number',\n",
    "        r'\\bhouse no\\b': 'house number',\n",
    "        r'\\bhse no\\b': 'house number',\n",
    "        r'\\bhs no\\b': 'house number',\n",
    "        r'\\bhno\\b': 'house number',\n",
    "        r'\\bprop no\\b': 'property number',\n",
    "        r'\\bflat no\\b': 'flat number',\n",
    "        r'\\bunit no\\b': 'unit number',\n",
    "        r'\\bkh no\\b': 'khasra number',\n",
    "        r'\\bkhasra no\\b': 'khasra number',\n",
    "        r'\\bkhno\\b': 'khasra number',\n",
    "        r'\\bkhn\\b': 'khasra number',\n",
    "\n",
    "\n",
    "        r'\\bg no\\b': 'gali number',\n",
    "        r'\\bgali no\\b': 'gali number',\n",
    "        r'\\bgno\\b': 'gali number',\n",
    "        r'\\bgn\\b': 'gali number',\n",
    "\n",
    "        r'\\bpno\\b': 'plot number',\n",
    "\n",
    "        r'\\bextn\\b': 'extension',\n",
    "        r'\\bext\\b': 'extension',\n",
    "        r'\\bextnti\\b': 'extension',\n",
    "        \n",
    "\n",
    "        r'\\bngr\\b': 'nagar',\n",
    "\n",
    "\n",
    "        r'\\bph\\b': 'phase',\n",
    "        r'\\bphs\\b': 'phase',\n",
    "        r'\\bphase no\\b': 'phase number',\n",
    "\n",
    "        r'\\bro\\b': 'room',\n",
    "        r'\\brm\\b': 'room',\n",
    "        r'\\broom no\\b': 'room number',\n",
    "\n",
    "\n",
    "        r'\\brd\\b': 'road',\n",
    "        r'\\brod\\b': 'road',\n",
    "        r'\\bmarg\\b': 'road',\n",
    "        r'\\bst\\b': 'street',\n",
    "        r'\\bsq\\b': 'square',\n",
    "\n",
    "        r'\\bldg\\b': 'building',\n",
    "        r'\\bbldg no\\b': 'building number',\n",
    "        r'\\bblk\\b': 'block',\n",
    "        r'\\bbl\\b': 'block',\n",
    "\n",
    "        r'\\bapp\\b': 'apartment',\n",
    "        r'\\bapt\\b': 'apartment',\n",
    "        r'\\bfl\\b': 'floor',\n",
    "        r'\\bgr fl\\b': 'ground floor',\n",
    "        r'\\b1st fl\\b': 'first floor',\n",
    "        r'\\b2nd fl\\b': 'second floor',\n",
    "        r'\\b3rd fl\\b': 'third floor',\n",
    "\n",
    "        r'\\blc\\b': 'locality',\n",
    "        r'\\barea\\b': 'area',\n",
    "        r'\\bpincode\\b': 'pincode',\n",
    "        r'\\bpin code\\b': 'pincode',\n",
    "\n",
    "        r'\\bnr\\b': 'near',\n",
    "        r'\\bopp\\b': 'opposite',\n",
    "        r'\\bop\\b': 'opposite',\n",
    "        r'\\b(no)\\b': 'number', # General 'no' at the end, if not caught by specific patterns\n",
    "    }\n",
    "\n",
    "    # Sort keys by length (descending) to ensure longer, more specific matches are attempted first.\n",
    "    sorted_fuzzy_map_keys = sorted(FUZZY_MAP.keys(), key=len, reverse=True)\n",
    "\n",
    "    for abbr_pattern in sorted_fuzzy_map_keys:\n",
    "        full_form = FUZZY_MAP[abbr_pattern]\n",
    "        address_text = re.sub(abbr_pattern, full_form, address_text)\n",
    "\n",
    "    # Clean up multiple spaces that might result from replacements\n",
    "    address_text = re.sub(r'\\s+', ' ', address_text).strip()\n",
    "    return address_text\n",
    "\n",
    "import pandas as pd\n",
    "#Assuming df is your DataFrame with an 'address' column\n",
    "main_df=pd.read_csv(\"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\exp1.csv\")#add path to your dataset that contains address column and division column\n",
    "\n",
    "# Separate data by division, here we are making seperate clusters in each division thus we diviided the dataframe earlier \n",
    "df_d1 = main_df[main_df['Division'] == 'D1'].copy()\n",
    "df_d2 = main_df[main_df['Division'] == 'D2'].copy()\n",
    "\n",
    "df_d1['CUSTADDR'] = df_d1['CUSTADDR'].apply(standardize_address_abbreviations_enhanced)\n",
    "df_d2['CUSTADDR'] = df_d2['CUSTADDR'].apply(standardize_address_abbreviations_enhanced)\n",
    "print(df_d1.head(10))\n",
    "print(df_d2.head(10))\n",
    "print(df_d2.describe())\n",
    "print(df_d1.describe())\n",
    "#df_standardized.to_csv(\"standardized_address_regex_based2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3322948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-APARTMENT', 'B-AREA', 'B-BLOCK', 'B-FLOOR', 'B-HOUSE_NUMBER', 'B-KHASRA_NUMBER', 'B-LANDMARK', 'B-LOCALITY', 'B-LOCALITY2', 'B-PINCODE', 'B-PLOT_NUMBER', 'B-ROAD_DETAILS', 'B-VILLAGE', 'I-APARTMENT', 'I-AREA', 'I-BLOCK', 'I-FLOOR', 'I-HOUSE_NUMBER', 'I-KHASRA_NUMBER', 'I-LANDMARK', 'I-LOCALITY', 'I-LOCALITY2', 'I-PLOT_NUMBER', 'I-ROAD_DETAILS', 'I-VILLAGE', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3361/3361 [1:38:40<00:00,  1.76s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "def batched_ner_out(raw_addresses):\n",
    "    raw_addresses = raw_addresses.tolist()\n",
    "    #print(raw_addresses)\n",
    "    print(label_list1)\n",
    "\n",
    "    # Predict in batches (not all at once)\n",
    "    batch_results = []\n",
    "    for chunk in tqdm(range(0, len(raw_addresses),64)):\n",
    "        batch = raw_addresses[chunk:chunk+64]\n",
    "        batch_results.extend(ner_pipe(batch))\n",
    "    print('batch results',batch_results)\n",
    "    return batch_results\n",
    "\n",
    "ner_df1=batched_ner_out(df_d1['CUSTADDR']) \n",
    "ner_df2=batched_ner_out(df_d2['CUSTADDR'])\n",
    "#ner_df2.head(2)\n",
    "print(ner_pipe(\"House No. 17-B, Plot No. 45, Old Plot No. 22, Third Floor, Gali No. 4, Street No. 2, Mohalla Shyam Nagar, Main Road, KH No. 78/3/2, B Block, Shyam Residency Apartment, Near Shiv Mandir, Shyam Nagar, Laxmi Nagar, Patparganj Village, Delhi - 110092\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db11374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✏️ 1) Edit this list if you add / drop labels later\n",
    "FIELDS = [\n",
    "    \"HOUSE_NUMBER\", \"PLOT_NUMBER\", \"FLOOR\", \"ROAD_DETAILS\", \"KHASRA_NUMBER\",\n",
    "    \"BLOCK\", \"APARTMENT\", \"LANDMARK\", \"LOCALITY\", \"AREA\", \"LOCALITY2\",\n",
    "    \"VILLAGE\", \"PINCODE\"\n",
    "]\n",
    "\n",
    "# 🔧 2) Utility to stitch sub‑words: \"##no\" → \"no\" (attach to previous token)\n",
    "def join_word(prev, cur):\n",
    "    if cur.startswith(\"##\"):\n",
    "        return prev + cur[2:]        # no space, drop ##\n",
    "    if prev.endswith(\"-\") or prev.endswith(\"/\"):\n",
    "        return prev + cur            # e.g. \"23/\" + \"A\"\n",
    "    if prev:                         # normal case: add a space\n",
    "        return prev + \" \" + cur\n",
    "    return cur                       # first token\n",
    "\n",
    "# 🏗️ 3) Convert one NER output list → dict of fields\n",
    "def convert_single_output(entity_list):\n",
    "    row = defaultdict(str)\n",
    "\n",
    "    for ent in entity_list:\n",
    "        key  = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"]\n",
    "        if row[key]:\n",
    "            row[key] = join_word(row[key], word)\n",
    "        else:\n",
    "            row[key] = word.lstrip(\"##\")  # first token\n",
    "\n",
    "    # 🌐 Compose complete address (simple comma join)\n",
    "    ordered_parts = [row[f] for f in FIELDS if row[f]]\n",
    "    if row[\"PINCODE\"]:\n",
    "        ordered_parts.append(f\"Delhi -{row['PINCODE']}\")   \n",
    "    row[\"complete_address\"] = \", \".join(ordered_parts)\n",
    "\n",
    "    return row\n",
    "\n",
    "# 🚀 4) Run on your whole batch_results list\n",
    "parsed_rows = [convert_single_output(addr_entities) for addr_entities in ner_df1]\n",
    "df1 = pd.DataFrame(parsed_rows)\n",
    "parsed_rows = [convert_single_output(addr_entities) for addr_entities in ner_df2]\n",
    "df2 = pd.DataFrame(parsed_rows)\n",
    "\n",
    "# 🔖 5) Re‑order columns and save\n",
    "df1= df1[FIELDS + [\"complete_address\"]]\n",
    "df_d1 = pd.concat([df1, df_d1], axis=0, ignore_index=True)\n",
    "\n",
    "df2= df2[FIELDS + [\"complete_address\"]]\n",
    "df_d2 = pd.concat([df2, df_d2], axis=0, ignore_index=True)\n",
    "#df.to_csv(\"parsed_normalized_address.csv\", index=False)\n",
    "\n",
    "print(\"added parsed addresses to the original df\")\n",
    "print(df_d1.head())\n",
    "print(df_d2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb061e",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "fiased based hierachial clustering approach but wiht my murils encoder layer embeddings\n",
    "partially works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b398ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Windows\\Temp\\ipykernel_23808\\4147316658.py:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  model_path = \"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\muril_120k_5epochs\"\n",
      "d:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for addresses...\n",
      "Generated embeddings shape: (7, 768)\n",
      "Example embedding for 'J - 3 / 118 - J , Laxmi Nagar Extension , Delhi - 110092':\n",
      "[ 0.00938649  0.05698579 -0.0980868   0.02605196 -0.10472456  0.02071202\n",
      " -0.03488051 -0.03788073 -0.0597924  -0.03322206]...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import numpy as np # For numerical operations\n",
    "\n",
    "# ✅ Load the saved model + tokenizer\n",
    "model_path = \"D:\\ML\\grok_adddress_parser\\grok_address_parser-env\\muril_120k_5epochs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval() # Set model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# --- Function to get embeddings ---\n",
    "def get_sentence_embedding(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Generates a sentence embedding for a given text using the fine-tuned MURIL model.\n",
    "    We use the [CLS] token embedding from the last hidden states as the sentence representation.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        # Get outputs without the classification head\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # The last hidden state contains embeddings from the BERT encoder\n",
    "    # outputs.hidden_states is a tuple, last element is the last layer's hidden states\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "    # Typically, the embedding of the [CLS] token (first token) is used as the sentence embedding.\n",
    "    # You can also consider mean-pooling all token embeddings for the sentence.\n",
    "    # For address data, [CLS] token embedding is a common choice.\n",
    "    cls_embedding = last_hidden_states[:, 0, :].squeeze() # Take [CLS] token (index 0) embedding\n",
    "\n",
    "    return cls_embedding.cpu().numpy() # Move to CPU and convert to NumPy array\n",
    "\n",
    "# --- Example Usage ---\n",
    "address_texts = [\n",
    "    \"J - 3 / 118 - J , Laxmi Nagar Extension , Delhi - 110092\",\n",
    "    \"J - 3 / 163 , Laxmi Nagar J Extension , Delhi - 110092\",\n",
    "    \"G - 5 / 58 , GALI NO - 1 , SONIA VIHAR , Delhi - 110094\",\n",
    "    \"D - 17 , GALI - 13 , PREM VIHAR , SHIV VIHAR KARAWAL NAGAR , Delhi - 110094\",\n",
    "    \"A-4 Shalimar Garden\",\n",
    "    \"A-4 Shalimar Garden Extension\",\n",
    "    \"H No - 5 / 195 , Lalita Park , Laxmi Nagar , Delhi - 110092\"\n",
    "]\n",
    "\n",
    "all_embeddings = []\n",
    "print(\"Generating embeddings for addresses...\")\n",
    "for text in address_texts:\n",
    "    embedding = get_sentence_embedding(text, model, tokenizer, device)\n",
    "    all_embeddings.append(embedding)\n",
    "\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "print(f\"Generated embeddings shape: {embeddings_array.shape}\") # (num_addresses, embedding_dim)\n",
    "print(f\"Example embedding for '{address_texts[0]}':\\n{embeddings_array[0][:10]}...\") # Print first 10 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328f73dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the Faiss index: 7\n",
      "\n",
      "Nearest neighbors for the first address:\n",
      "Distances: [[0.         0.00386813 0.00546896 0.02231091 0.04010084]]\n",
      "Indices: [[0 6 1 3 5]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Ensure embeddings are float32, as Faiss typically prefers it\n",
    "embeddings_array = embeddings_array.astype('float32')\n",
    "embedding_dim = embeddings_array.shape[1]\n",
    "\n",
    "# Create a Faiss index\n",
    "# IndexFlatL2: Simple L2 (Euclidean) distance, exact search. Good for smaller sets or when high precision is critical.\n",
    "# For very large datasets (millions+), you'd use approximate indexes like IndexIVFFlat or IndexHNSW.\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add your embeddings to the index\n",
    "index.add(embeddings_array)\n",
    "\n",
    "print(f\"Number of vectors in the Faiss index: {index.ntotal}\")\n",
    "\n",
    "# Find the 5 nearest neighbors for the first address\n",
    "D, I = index.search(embeddings_array[:1], k=5)\n",
    "# D: Distances (L2 distance), I: Indices of nearest neighbors\n",
    "print(\"\\nNearest neighbors for the first address:\")\n",
    "print(f\"Distances: {D}\")\n",
    "print(f\"Indices: {I}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8278f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating cosine similarity matrix for embeddings...\n",
      "Diagonal of distance matrix set to zero.\n",
      "\n",
      "Linkage matrix (Z) created successfully.\n",
      "First 5 rows of Z_embeddings:\n",
      " [[0.00000000e+00 6.00000000e+00 9.82403755e-04 2.00000000e+00]\n",
      " [1.00000000e+00 7.00000000e+00 1.99000075e-03 3.00000000e+00]\n",
      " [4.00000000e+00 5.00000000e+00 2.95031071e-03 2.00000000e+00]\n",
      " [2.00000000e+00 3.00000000e+00 4.40824032e-03 2.00000000e+00]\n",
      " [8.00000000e+00 1.00000000e+01 1.13768855e-02 5.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clusters formed at distance threshold 0.005:\n",
      "Cluster 1: ['C5', 'C6']\n",
      "Cluster 2: ['C1', 'C2', 'C7']\n",
      "Cluster 3: ['C3', 'C4']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram,fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate cosine similarity matrix (0 to 1, higher is more similar)\n",
    "print(\"\\nCalculating cosine similarity matrix for embeddings...\")\n",
    "# For very large datasets, sklearn's pairwise can be slow. Faiss or custom batched GPU calculations are better.\n",
    "# However, for 30k, it might be feasible on CPU.\n",
    "cosine_sim_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Convert similarity to distance (0 to 1, higher is more distant)\n",
    "distance_matrix_embeddings = 1 - cosine_sim_matrix\n",
    "# --- FIX: Explicitly set the diagonal to zero ---\n",
    "np.fill_diagonal(distance_matrix_embeddings, 0)\n",
    "print(\"Diagonal of distance matrix set to zero.\")\n",
    "\n",
    "# --- Then use SciPy's hierarchical clustering (as you've learned) ---\n",
    "condensed_distance_embeddings = squareform(distance_matrix_embeddings)\n",
    "Z_embeddings = linkage(condensed_distance_embeddings, method='ward') # Or 'average', 'complete'\n",
    "\n",
    "print(\"\\nLinkage matrix (Z) created successfully.\")\n",
    "print(\"First 5 rows of Z_embeddings:\\n\", Z_embeddings[:5])\n",
    "\n",
    "# --- Plot dendrogram (assuming you have labels, e.g., customer_ids_for_embeddings) ---\n",
    "# For this example, let's create dummy labels\n",
    "customer_ids_for_embeddings = [f'C{i+1}' for i in range(embeddings_array.shape[0])]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Hierarchical Clustering Dendrogram for Embeddings')\n",
    "plt.xlabel('Customer ID')\n",
    "plt.ylabel('Distance (1 - Cosine Similarity)')\n",
    "\n",
    "dendrogram(\n",
    "    Z_embeddings,\n",
    "    labels=customer_ids_for_embeddings,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    color_threshold=0.05, # Adjust this based on your dendrogram\n",
    "    above_threshold_color='blue'\n",
    ")\n",
    "plt.axhline(y=0.005, color='r', linestyle='--', label='Example Cut-off Distance (0.3)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Optional: Extract Clusters at a Specific Threshold ---\n",
    "max_d = 0.005 # Example threshold\n",
    "clusters_embeddings = fcluster(Z_embeddings, max_d, criterion='distance')\n",
    "\n",
    "print(f\"\\nClusters formed at distance threshold {max_d}:\")\n",
    "for cluster_id in sorted(np.unique(clusters_embeddings)):\n",
    "    cluster_members = [customer_ids_for_embeddings[i] for i, cid in enumerate(clusters_embeddings) if cid == cluster_id]\n",
    "    print(f\"Cluster {cluster_id}: {cluster_members}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b7bf6",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "EMBEDDINGS ON WHOLE SENTENCE using paraphrase-multilingual-MiniLM-L12-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d68357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# === Load and Preprocess ===\n",
    "df = pd.read_csv('D:/ML/grok_adddress_parser/grok_address_parser-env/clustering_test_subset_extended.csv')\n",
    "df.rename(columns={'test_id': 'customer_id'}, inplace=True)\n",
    "\n",
    "columns_to_merge = [\n",
    "    'house_number', 'plot_number', 'road_details', 'locality',\n",
    "    'area', 'locality2', 'village'\n",
    "]\n",
    "\n",
    "# Combine fields into a structured string\n",
    "df['address_extract'] = df[columns_to_merge].fillna('').apply(lambda row: ', '.join(filter(None, row)), axis=1)\n",
    "df['address_extract'] = df['address_extract'].str.replace(r'\\bdelhi\\b', '', case=False, regex=True).str.strip(', ').str.strip()\n",
    "\n",
    "# Fill missing locality values for grouping\n",
    "df['locality'] = df['locality'].fillna('unknown').str.lower().str.strip()\n",
    "\n",
    "# === Load Embedding Model ===\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "#intfloat/multilingual-e5-small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Group by Locality ===\n",
    "locality_groups = df.groupby('locality')\n",
    "locality_faiss_indices = {}\n",
    "cluster_db = []\n",
    "\n",
    "for locality, group in locality_groups:\n",
    "    print(f\"Processing locality: {locality} | Records: {len(group)}\")\n",
    "\n",
    "    addresses = group['address_extract'].tolist()\n",
    "    customer_ids = group['customer_id'].tolist()\n",
    "\n",
    "    # Embed and normalize with progress bar\n",
    "    embeddings = np.vstack([model.encode(addr, normalize_embeddings=True) for addr in tqdm(addresses, desc=f\"Embedding {locality}\")])\n",
    "\n",
    "    # Create FAISS index\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # cosine similarity\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Perform similarity search\n",
    "    similarity_threshold = 0.95\n",
    "    k = 10\n",
    "    _, indices = index.search(embeddings, k)\n",
    "\n",
    "    visited = set()\n",
    "    locality_clusters = []\n",
    "\n",
    "    for i, neighbors in enumerate(indices):\n",
    "        if customer_ids[i] in visited:\n",
    "            continue\n",
    "\n",
    "        cluster = set()\n",
    "        for j in neighbors:\n",
    "            if j == -1:\n",
    "                continue\n",
    "            sim = np.dot(embeddings[i], embeddings[j])\n",
    "            if sim >= similarity_threshold:\n",
    "                cluster.add(customer_ids[j])\n",
    "                visited.add(customer_ids[j])\n",
    "\n",
    "        if cluster:\n",
    "            locality_clusters.append({\n",
    "                \"cluster_id\": f\"{locality[:5]}_{i}\",\n",
    "                \"locality\": locality,\n",
    "                \"canonical_address\": addresses[i],\n",
    "                \"customer_ids\": list(cluster)\n",
    "            })\n",
    "\n",
    "    cluster_db.extend(locality_clusters)\n",
    "    locality_faiss_indices[locality] = index  # optional if you want to reuse\n",
    "\n",
    "# === Save Cluster Output ===\n",
    "os.makedirs(\"cluster_output\", exist_ok=True)\n",
    "pd.DataFrame(cluster_db).to_json(\"cluster_output/clusters.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(f\"Total clusters formed: {len(cluster_db)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d055041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "near\n",
      "laxmi\n",
      "nagar\n",
      "gali\n",
      "number\n",
      "mandawali\n",
      "shakarpur\n",
      "west\n",
      "mandir\n",
      "pandav\n",
      "delhi\n",
      "fazalpur\n",
      "vinod\n",
      "extension\n",
      "road\n",
      "patpar\n",
      "indra\n",
      "prastha\n",
      "guru\n",
      "ganesh\n",
      "park\n",
      "pur\n",
      "ganj\n",
      "ram\n",
      "school\n",
      "vihar\n",
      "main\n",
      "angad\n",
      "lalita\n",
      "gazi\n",
      "opposite\n",
      "preet\n",
      "madhu\n",
      "village\n",
      "dass\n",
      "kunj\n",
      "colony\n",
      "chander\n",
      "east\n",
      "krishna\n",
      "chowk\n",
      "railway\n",
      "ramesh\n",
      "dairy\n",
      "masjid\n",
      "mother\n",
      "market\n",
      "j\n",
      "shiv\n",
      "durga\n",
      "wali\n",
      "mangal\n",
      "back\n",
      "behind\n",
      "shankar\n",
      "hanuman\n",
      "public\n",
      "side\n",
      "joshi\n",
      "gurudwara\n",
      "by\n",
      "mohalla\n",
      "nanak\n",
      "ka\n",
      "bazar\n",
      "krishan\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"D:/ML/grok_adddress_parser/grok_address_parser-env/d1_structured_parsed.csv\")\n",
    "# For example, assume addresses are in df[\"address\"]\n",
    "columns_to_merge=['LOCALITY','LOCALITY2','ROAD_DETAILS','AREA','LANDMARK','VILLAGE']\n",
    "df['address_extract'] = df[columns_to_merge].fillna('').apply(lambda row: ', '.join(filter(None, row)), axis=1)\n",
    "\n",
    "tokens = [tok for addr in df['address_extract'] for tok in addr.split()]\n",
    "common_tokens = Counter(tokens).most_common(100)\n",
    "for token, freq in common_tokens:\n",
    "    if token.isalpha() and freq > 50:\n",
    "        print(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1131a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧼 Total unique clean tokens: 10585\n",
      "['a', 'aa', 'aachar', 'aadar', 'aadarsh', 'aadersh', 'aaditya', 'aagarwal', 'aahuja', 'aakash', 'aakashar', 'aakriti', 'aakrriti', 'aalaha', 'aamar', 'aamarpali', 'aanar', 'aaprtment', 'aapt', 'aas', 'aasa', 'aasha', 'aashan', 'aashirvad', 'aashirwad', 'aashram', 'aata', 'ab', 'abad', 'abadi', 'abadram', 'abb', 'abbdula', 'abbdulla', 'abc', 'abcd', 'abddulla', 'abdekar', 'abdul', 'abdula', 'abdulah', 'abdulha', 'abdulhha', 'abdulla', 'abdullaah', 'abdullah', 'abdullaha', 'abdullh', 'abdullha', 'abetker', 'abfullah', 'abha', 'abhdulla', 'abmdekar', 'abodi', 'about', 'above', 'abp', 'abrol', 'abv', 'ac', 'academy', 'accociate', 'account', 'acd', 'acha', 'achar', 'acharya', 'achhar', 'achid', 'achivers', 'achool', 'acmp', 'acxar', 'ad', 'ada', 'adan', 'adarash', 'adaresh', 'adars', 'adarsh', 'adarshbalbhartipublic', 'adarshpublice', 'add', 'adda', 'additi', 'address', 'adersh', 'adesh', 'adh', 'adhar', 'adharash', 'adharshila', 'adhikari', 'adhikunj', 'adhyatmic', 'adidas', 'aditay', 'aditaya', 'aditi']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_clean_tokens(df, columns):\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for col in columns:\n",
    "        for val in df[col].dropna():\n",
    "            if isinstance(val, str):\n",
    "                # Extract words only (skip numbers and special characters)\n",
    "                tokens = re.findall(r'\\b[a-zA-Z]+\\b', val)\n",
    "                unique_tokens.update([token.lower() for token in tokens])  # normalize to lowercase\n",
    "\n",
    "    return sorted(unique_tokens)\n",
    "\n",
    "# Example usage\n",
    "columns = ['ROAD_DETAILS', 'APARTMENT',\n",
    "           'LANDMARK', 'LOCALITY', 'AREA', 'LOCALITY2', 'VILLAGE']\n",
    "\n",
    "# Assuming you have read your CSV as `df`\n",
    "# df = pd.read_csv(\"your_file.csv\")\n",
    "clean_tokens = extract_clean_tokens(df, columns)\n",
    "\n",
    "print(f\"🧼 Total unique clean tokens: {len(clean_tokens)}\")\n",
    "print(clean_tokens[:100])  # Preview first 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Sample list of tokens (replace this with your 20k list)\n",
    "\n",
    "# Simulate frequencies (replace with actual token counts)\n",
    "token_counts = Counter({\n",
    "    'ambedkar': 100,\n",
    "    'ambdekar': 2,\n",
    "    'ambdkar': 3,\n",
    "    'ambedakr': 5,\n",
    "    'aa': 20,\n",
    "    'i': 30,\n",
    "    'j': 25,\n",
    "    'o': 22,\n",
    "    'xyz': 1\n",
    "})\n",
    "\n",
    "def is_safe_token(token):\n",
    "    return len(token) <= 2 and token.isalnum()\n",
    "\n",
    "safe_tokens = set([t for t in clean_tokens if is_safe_token(t)])\n",
    "correctable_tokens = [t for t in clean_tokens if t not in safe_tokens]\n",
    "\n",
    "all_tokens = correctable_tokens + list(safe_tokens)\n",
    "\n",
    "# Load SBERT multilingual embedding model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(correctable_tokens, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Cluster semantically similar tokens\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.0)\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "# Group tokens by cluster\n",
    "clusters = {}\n",
    "for token, label in zip(correctable_tokens, labels):\n",
    "    clusters.setdefault(label, []).append(token)\n",
    "\n",
    "# Map tokens in each cluster to most frequent one\n",
    "replacement_map = {}\n",
    "for group in clusters.values():\n",
    "    representative = max(group, key=lambda w: token_counts[w])\n",
    "    for token in group:\n",
    "        replacement_map[token] = representative\n",
    "\n",
    "# Identity mappings for safe tokens\n",
    "for token in safe_tokens:\n",
    "    replacement_map[token] = token\n",
    "\n",
    "# Output corrected mapping\n",
    "mapping_df = pd.DataFrame({\n",
    "    'original_token': all_tokens,\n",
    "    'corrected_token': [replacement_map[tok] for tok in all_tokens]\n",
    "})\n",
    "\n",
    "# Optional: Save mapping\n",
    "mapping_df.to_csv(\"token_corrections.csv\", index=False)\n",
    "print(mapping_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f5996d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1594\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1593\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1596\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m   1600\u001b[39m bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[31mValueError\u001b[39m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     30\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mai4bharat/indic-bert\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m model = AutoModel.from_pretrained(model_name)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_embedding\u001b[39m(text):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1032\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1029\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2025\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2022\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2023\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2278\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2276\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2277\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2278\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2280\u001b[39m     logger.info(\n\u001b[32m   2281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2283\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\models\\albert\\tokenization_albert_fast.py:112\u001b[39m, in \u001b[36mAlbertTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     90\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it and\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# is included in the raw text, there should be a match in a non-normalized sentence.\u001b[39;00m\n\u001b[32m    106\u001b[39m     mask_token = (\n\u001b[32m    107\u001b[39m         AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, normalized=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    108\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    109\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m    110\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_accents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_accents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mself\u001b[39m.do_lower_case = do_lower_case\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m.remove_space = remove_space\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\grok_adddress_parser\\grok_address_parser-env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1735\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1736\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1737\u001b[39m     ).converted()\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# ── Step 1: Load Data ───────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(\"D:/ML/grok_adddress_parser/grok_address_parser-env/exp1.csv\")\n",
    "df = df.iloc[:20000]\n",
    "ADDRESS_COL = \"CUSTADDR\"\n",
    "\n",
    "# ── Step 2: Extract “meaningful” tokens (only letters, length>2) ────────────────\n",
    "def extract_meaningful_tokens(text):\n",
    "    text = re.sub(r'\\b\\d{6}\\b', '', text.lower())        # drop pin\n",
    "    return re.findall(r'\\b[a-zA-Z]{3,}\\b', text)         # only 3+ letter words\n",
    "\n",
    "df['tokens'] = df[ADDRESS_COL].map(extract_meaningful_tokens)\n",
    "\n",
    "# ── Step 3: Build vocabulary + counts ─────────────────────────────────────────\n",
    "all_tokens   = [tok for toks in df['tokens'] for tok in toks]\n",
    "token_counts = Counter(all_tokens)\n",
    "unique_tokens = list(token_counts.keys())\n",
    "\n",
    "# ── Step 4: Embed tokens with SBERT ───────────────────────────────────────────\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"ai4bharat/indic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "embeddings = get_embedding(unique_tokens)\n",
    "\n",
    "# ── Step 5: Compute Cosine‑distance matrix ─────────────────────────────────────\n",
    "D = cosine_distances(embeddings)  # shape (V, V)\n",
    "\n",
    "# ── Step 6: Agglomerative clustering on precomputed distances ─────────────────\n",
    "clusterer = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    metric='precomputed',       # <- replaces 'affinity'\n",
    "    linkage='average',\n",
    "    distance_threshold=0.3\n",
    ")\n",
    "\n",
    "labels = clusterer.fit_predict(D)\n",
    "\n",
    "# ── Step 7: Build replacement map (most‑frequent in each cluster) ────────────\n",
    "clusters = {}\n",
    "for tok, lab in zip(unique_tokens, labels):\n",
    "    clusters.setdefault(lab, []).append(tok)\n",
    "\n",
    "replacement_map = {}\n",
    "for group in clusters.values():\n",
    "    # pick the *most frequent* token as canonical\n",
    "    canonical = max(group, key=lambda t: token_counts[t])\n",
    "    for t in group:\n",
    "        replacement_map[t] = canonical\n",
    "\n",
    "# ── Step 8: Apply replacements back into the full address ────────────────────\n",
    "def correct_address(text, correction_map):\n",
    "    words = text.split()\n",
    "    out = []\n",
    "    for w in words:\n",
    "        cleaned = re.sub(r'[^a-zA-Z]', '', w.lower())\n",
    "        if cleaned in correction_map and correction_map[cleaned] != cleaned:\n",
    "            # debug print to confirm replacements\n",
    "            print(f\"🔁 {cleaned} → {correction_map[cleaned]}\")\n",
    "            # preserve original delimiters/punctuation\n",
    "            new = re.sub(cleaned, correction_map[cleaned], w, flags=re.IGNORECASE)\n",
    "            out.append(new)\n",
    "        else:\n",
    "            out.append(w)\n",
    "    return \" \".join(out)\n",
    "\n",
    "df['corrected_address'] = df[ADDRESS_COL].map(lambda txt: correct_address(txt, replacement_map))\n",
    "\n",
    "# ── Step 9: Export your spell‑check map & inspect ──────────────────────────────\n",
    "corrections_df = (\n",
    "    pd.DataFrame([\n",
    "        {'original_token': o, 'corrected_token': c}\n",
    "        for o, c in replacement_map.items()\n",
    "        if o != c\n",
    "    ])\n",
    "    .sort_values('corrected_token')\n",
    ")\n",
    "print(\"=== Top 20 Corrections ===\")\n",
    "print(corrections_df.head(20))\n",
    "corrections_df.to_csv(\"token_spellcheck_map.csv\", index=False)\n",
    "\n",
    "# ── Step 10: Inspect sample before/after ──────────────────────────────────────\n",
    "print(df[[ADDRESS_COL, 'corrected_address']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a35cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_address(text, correction_map):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        # Normalize for matching (lowercase, strip punctuations)\n",
    "        cleaned = re.sub(r'[^a-zA-Z]', '', word.lower())\n",
    "        # If word matches correction, replace with canonical token (preserve casing)\n",
    "        if cleaned in correction_map:\n",
    "            corrected = correction_map[cleaned]\n",
    "            # Maintain original punctuation\n",
    "            corrected_words.append(word.lower().replace(cleaned, corrected))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "df['corrected_address'] = df[address_col].apply(lambda x: correct_address(x, replacement_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d801b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   original_token corrected_token\n",
      "70          janta           anagd\n",
      "37          nanak           ankur\n",
      "39          gokal           ankur\n",
      "38         govind           ankur\n",
      "40         gautam           ankur\n",
      "41          janki           ankur\n",
      "9          patpar           basti\n",
      "71         bhatra           bhati\n",
      "45      gokulpuri        brijpuri\n",
      "46       rajdhani        brijpuri\n",
      "20          pusta         chauhan\n",
      "24            tek            dass\n",
      "22            som            dass\n",
      "23       dayanand            dass\n",
      "18          param           durga\n",
      "19         mukand           durga\n",
      "44          santu          gandhi\n",
      "54       gokalpur       gokalpuri\n",
      "27         makund          gujran\n",
      "28          kallu          gujran\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame of corrections\n",
    "corrections_df = pd.DataFrame([\n",
    "    {'original_token': orig, 'corrected_token': corrected}\n",
    "    for orig, corrected in replacement_map.items()\n",
    "    if orig != corrected  # Only show where correction actually happened\n",
    "])\n",
    "\n",
    "# Optional: Sort by corrected_token or frequency\n",
    "corrections_df = corrections_df.sort_values(by='corrected_token')\n",
    "\n",
    "# Show top 20 corrections\n",
    "print(corrections_df.head(20))\n",
    "\n",
    "# Save for inspection\n",
    "corrections_df.to_csv(\"token_spellcheck_map.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e84551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grok_address_parser-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
